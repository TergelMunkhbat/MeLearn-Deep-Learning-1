{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "211f9c01",
   "metadata": {},
   "source": [
    "# Deep Learning - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649f7822",
   "metadata": {},
   "source": [
    "## Chapter 4: Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592d6d6e",
   "metadata": {},
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a24843",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f0ca82",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fca357a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a494e6",
   "metadata": {},
   "source": [
    "<img src=\"images/layer.png\" alt=\"Drawing\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b879cc",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83e1ef8",
   "metadata": {},
   "source": [
    "#### Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d55c27a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Linear:\n",
    "    \"\"\"Representing a neural network layer\"\"\"\n",
    "    \n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        \"\"\"Initlize weights and bias\"\"\"\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_outputs)\n",
    "        self.biases = np.zeros((1, n_outputs))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        It multiplies the inputs by the weights \n",
    "        and then sums them, and then sums bias.\n",
    "        \"\"\"\n",
    "        #To calculate gradient, remembering input values\n",
    "        self.inputs = inputs\n",
    "        #Calculate outputs' values\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        \"\"\"Gradient with respect to parameters and input\"\"\"\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        self.dresults = np.dot(dvalues, self.weights.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05a274b",
   "metadata": {},
   "source": [
    "#### Dropout Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efe7222d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dropout:\n",
    "    \"\"\"Representing a dropout layer\"\"\"\n",
    "    \n",
    "    def __init__(self, rate):\n",
    "        \"\"\"Initlize the success rate of binomial distribution\"\"\"\n",
    "        self.rate = 1 - rate\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Generate the scaled mask and then\n",
    "        apply the mask to the inputs values\n",
    "        \"\"\"\n",
    "        #Generate the scaled mask\n",
    "        self.scaled_mask = np.random.binomial(1, self.rate,\n",
    "                                             size=inputs.shape) / self.rate\n",
    "        #Calculate outputs' values\n",
    "        self.output = inputs * self.scaled_mask\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        \"\"\"\n",
    "        Gradient with respect to inputs, and then\n",
    "        multiply the dvalues accroding to the chain rule\n",
    "        \"\"\"\n",
    "        self.dresults = self.scaled_mask * dvalues    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09103fd1",
   "metadata": {},
   "source": [
    "### Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149c787f",
   "metadata": {},
   "source": [
    "#### Softmax Activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31d7786",
   "metadata": {},
   "source": [
    "<img src=\"images/softmax.png\" alt=\"Drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "820a4706",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax:\n",
    "    \"\"\"Softmax activation\"\"\"\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        \n",
    "        #Compute e^x for each element of inputs\n",
    "        #Due to the overflow error, \n",
    "        #Maximum value of per sample subtract from each row\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "                                           keepdims=True))\n",
    "        \n",
    "        #Normalize them for each batch\n",
    "        self.output = exp_values / np.sum(exp_values, \n",
    "                                          axis=1, keepdims=True)\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        \"\"\"Gradient softmax\"\"\"\n",
    "        \n",
    "        #Initialize an array\n",
    "        self.dresults = np.zeros(dvalues.shape)\n",
    "        \n",
    "        for i in range(len(dvalues)):\n",
    "            #Reshape the single output\n",
    "            single_output = self.output[i].reshape(-1, 1)\n",
    "            \n",
    "            #Calculate Jacobian matrix of the single output\n",
    "            jacobian_matrix = np.diagflat(single_output) - \\\n",
    "                                np.dot(single_output, single_output.T)\n",
    "            \n",
    "            #Multiply the Jacobian matrix by the loss function derivative\n",
    "            self.dresults[i] = np.dot(jacobian_matrix, dvalues[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e25066",
   "metadata": {},
   "source": [
    "#### ReLU Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "000d3640",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLU:\n",
    "    \"\"\"ReLU activation\"\"\"\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        \n",
    "        #To calculate gradient, remembering input values\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        #Calculate outputs' values\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        \"\"\"Backward pass\"\"\"\n",
    "        \n",
    "        self.dresults = self.inputs > 0\n",
    "        self.dresults = self.dresults * dvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325b26d3",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b268858f",
   "metadata": {},
   "source": [
    "#### MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5529a656",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_MSE():\n",
    "    \"\"\"MSE Loss function\"\"\"\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"Forward pass\"\"\"     \n",
    "        error = np.mean((y_pred - y_true) ** 2)\n",
    "        return error\n",
    "    \n",
    "    def backward(self, y_pred, y_true):\n",
    "        \"\"\"Derivative of MSE with respect to preds\"\"\"\n",
    "        \n",
    "        #Number of samples\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        #Number of output nodes\n",
    "        outputs = len(y_pred[0])\n",
    "        \n",
    "        #Derivative of MSE\n",
    "        self.dresults = 2 * (y_pred - y_true) / (outputs * samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8e8970",
   "metadata": {},
   "source": [
    "#### Categorical Cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07e2403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_CategoricalCrossEntropy():\n",
    "    \"\"\"Cross entropy Loss function\"\"\"\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        y_pred += 1e-10\n",
    "        y_pred = np.clip(y_pred, None, 1)\n",
    "        true_prediction = np.sum(y_pred * y_true, axis=1)\n",
    "        error = np.mean(-np.log(true_prediction)) \n",
    "        return error\n",
    "    \n",
    "    def backward(self, y_pred, y_true):\n",
    "        \"\"\"Derivative of CCE with respect to preds\"\"\"\n",
    "        \n",
    "        #Number of samples\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        self.dresults = -y_true / (y_pred * samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38650268",
   "metadata": {},
   "source": [
    "#### Categorical Cross-entropy + Softmax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24017a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_CategoricalCrossEntropy_Activation_SoftMax:\n",
    "    \"\"\"Cateogircal cross entropy loss and SoftMax function\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize Softmax and CCE loss\"\"\"\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossEntropy()\n",
    "        \n",
    "    def forward(self, inputs, y_true):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        self.activation.forward(inputs)\n",
    "        return self.loss.forward(self.activation.output, y_true)\n",
    "    \n",
    "    def backward(self, y_pred, y_true):\n",
    "        \"\"\"Gradient of Categorical cross entropy + Softmax activation\"\"\"\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        self.dresults = (y_pred - y_true) / samples        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae60fe79",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e5b134b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy_Categorical:\n",
    "    \"\"\"Accuracy calculation for classification\"\"\"\n",
    "    \n",
    "    def calculate(self, y_pred, y_true):\n",
    "        \"\"\"Calculate the accuracy\"\"\"\n",
    "        \n",
    "        true = np.argmax(y_true, axis=1)\n",
    "        pred = np.argmax(y_pred, axis=1)\n",
    "        comparisons = true == pred\n",
    "        \n",
    "        accuracy = np.mean(comparisons)\n",
    "        \n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d926f58",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8a3dfc",
   "metadata": {},
   "source": [
    "#### Gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4d350dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_GD:\n",
    "    \"\"\"Gradient descent optimizer\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=1., momentum=0):\n",
    "        \"\"\"Initialize hyperparameters\"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.momentum = momentum\n",
    "\n",
    "    def update_parameters(self, layer):\n",
    "        \"\"\"Update parameters\"\"\"\n",
    "\n",
    "        if self.momentum:\n",
    "            \n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            \n",
    "            weights_delta = self.momentum * layer.weight_momentums + \\\n",
    "                            layer.dweights * self.alpha\n",
    "            biases_delta = self.momentum * layer.bias_momentums + \\\n",
    "                            layer.dbiases * self.alpha\n",
    "            \n",
    "            layer.weight_momentums = weights_delta\n",
    "            layer.bias_momentums = biases_delta\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            weights_delta = layer.dweights * self.alpha\n",
    "            biases_delta = layer.dbiases * self.alpha\n",
    "        \n",
    "        #Update parameters\n",
    "        layer.weights -= weights_delta\n",
    "        layer.biases -= biases_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87dc2bc",
   "metadata": {},
   "source": [
    "#### AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c0ed23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_AdaGrad:\n",
    "    \"\"\"Gradient descent optimizer\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=1., epsilon=1e-10):\n",
    "        \"\"\"Initialize hyperparameters\"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def update_parameters(self, layer):\n",
    "        \"\"\"Update parameters\"\"\"\n",
    "            \n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        layer.weight_cache += layer.dweights ** 2\n",
    "        layer.bias_cache += layer.dbiases ** 2\n",
    "        \n",
    "        weights_delta = layer.dweights * self.alpha / \\\n",
    "                        (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        biases_delta =  layer.dbiases * self.alpha / \\\n",
    "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "        \n",
    "        #Update parameters\n",
    "        layer.weights -= weights_delta\n",
    "        layer.biases -= biases_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cff996",
   "metadata": {},
   "source": [
    "#### RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90e5922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_RMSprop:\n",
    "    \"\"\"Gradient descent optimizer\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0.01, epsilon=1e-10, rho=0.99):\n",
    "        \"\"\"Initialize hyperparameters\"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "\n",
    "    def update_parameters(self, layer):\n",
    "        \"\"\"Update parameters\"\"\"\n",
    "            \n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        layer.weight_cache = self.rho * layer.weight_cache + \\\n",
    "                        (1 - self.rho) * layer.dweights ** 2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + \\\n",
    "                        (1 - self.rho) * layer.dbiases ** 2\n",
    "        \n",
    "        weights_delta = layer.dweights * self.alpha / \\\n",
    "                        (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        biases_delta =  layer.dbiases * self.alpha / \\\n",
    "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "        \n",
    "        #Update parameters\n",
    "        layer.weights -= weights_delta\n",
    "        layer.biases -= biases_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9fab2d",
   "metadata": {},
   "source": [
    "#### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30ed509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_Adam:\n",
    "    \"\"\"Gradient descent optimizer\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0.001, epsilon=1e-7,\n",
    "                 beta_1=0.9, beta_2=0.999):\n",
    "        \"\"\"Initialize hyperparameters\"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        self.iterations = 0\n",
    "    \n",
    "    def update_parameters(self, layer):\n",
    "        \"\"\"Update parameters\"\"\"\n",
    "            \n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "        \n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + \\\n",
    "                                (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + \\\n",
    "                                (1 - self.beta_1) * layer.dbiases\n",
    "        \n",
    "        weight_momentums_corrected = layer.weight_momentums / \\\n",
    "                                (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / \\\n",
    "                                (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        \n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + \\\n",
    "                                (1 - self.beta_2) * layer.dweights ** 2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + \\\n",
    "                                (1 - self.beta_2) * layer.dbiases ** 2\n",
    "        \n",
    "        weight_cache_corrected = layer.weight_cache / \\\n",
    "                                (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / \\\n",
    "                                (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        \n",
    "        weights_delta = weight_momentums_corrected * self.alpha / \\\n",
    "                        (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "        biases_delta =  bias_momentums_corrected * self.alpha / \\\n",
    "                        (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "        \n",
    "        #Update parameters\n",
    "        layer.weights -= weights_delta\n",
    "        layer.biases -= biases_delta\n",
    "    \n",
    "    def post_update_parameters(self):\n",
    "        \"\"\"Increase iteration\"\"\"\n",
    "        \n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6ba5dc",
   "metadata": {},
   "source": [
    "### Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9912b52d",
   "metadata": {},
   "source": [
    "#### Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b97eddea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scaler_Standard:\n",
    "    \"\"\"Standard scaler\"\"\"\n",
    "    \n",
    "    def fit(self, data):\n",
    "        \"\"\"Find mean and std values\"\"\"\n",
    "        self.means = data.mean(axis=0)\n",
    "        self.stds = data.std(axis=0)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, data):\n",
    "        \"\"\"Transforming data\"\"\"\n",
    "        return (data - self.means) / self.stds\n",
    "    \n",
    "    def fit_transform(self, data):\n",
    "        \"\"\"Fit and transform data\"\"\"\n",
    "        return self.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c850cf2",
   "metadata": {},
   "source": [
    "#### MinMax Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e5c1fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scaler_MinMax:\n",
    "    \"\"\"MinMax scaler\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_range=(0,1)):\n",
    "        \"\"\"Initialize the feature range\"\"\"\n",
    "        self.low, self.high = feature_range\n",
    "    \n",
    "    def fit(self, data):\n",
    "        \"\"\"Find min and max values\"\"\"\n",
    "        self.min = data.min(axis=0)\n",
    "        self.max = data.max(axis=0)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, data):\n",
    "        \"\"\"Transforming data\"\"\"\n",
    "        data_std = (data - self.min) / (self.max - self.min)\n",
    "        return data_std * (self.high - self.low) + self.low\n",
    "    \n",
    "    def fit_transform(self, data):\n",
    "        \"\"\"Fit and transform data\"\"\"\n",
    "        return self.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa52189",
   "metadata": {},
   "source": [
    "#### Robust Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e867d610",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scaler_Robust:\n",
    "    \"\"\"Robust scaler\"\"\"\n",
    "    \n",
    "    def fit(self, data):\n",
    "        \"\"\"Find median and iqr values\"\"\"\n",
    "        self.medians = np.median(data, axis=0)\n",
    "        self.p75, self.p25 = np.percentile(data, [75 ,25], axis=0)\n",
    "        self.iqr = self.p75 - self.p25\n",
    "        return self\n",
    "    \n",
    "    def transform(self, data):\n",
    "        \"\"\"Transforming data\"\"\"\n",
    "        return (data - self.medians) / self.iqr\n",
    "    \n",
    "    def fit_transform(self, data):\n",
    "        \"\"\"Fit and transform data\"\"\"\n",
    "        return self.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3c0507",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7059a12c",
   "metadata": {},
   "source": [
    "### Construct Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b843abf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('T-shirt', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03a14d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    \"\"\"\n",
    "    Load the MNIST fashion dataset\n",
    "    Convert the labels into one-hot vectors\n",
    "    \"\"\"\n",
    "\n",
    "    labels = os.listdir(os.path.join(path))\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    for label in labels:\n",
    "        for file in os.listdir(os.path.join(path, label)):\n",
    "            image = cv2.imread(os.path.join(path, label, file),\n",
    "                                  cv2.IMREAD_UNCHANGED)\n",
    "            X.append(image)\n",
    "            Y.append(label)\n",
    "    \n",
    "    \n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y).astype('uint8')\n",
    "    Y = np.eye(len(labels))[Y].astype('uint8')\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b7862d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_dataset, train_val_labels = load_dataset('../dataset/train')\n",
    "test_dataset, test_labels = load_dataset('../dataset/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5b886cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829b3d72",
   "metadata": {},
   "source": [
    "#### Flatten the every image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cacffd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_dataset = train_val_dataset.reshape(len(train_val_dataset), -1)\n",
    "test_dataset = test_dataset.reshape(len(test_dataset), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29d2ae4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4516efe3",
   "metadata": {},
   "source": [
    "#### Data shuffling and splits to train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d885db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = np.array(range(len(train_val_dataset)))\n",
    "np.random.shuffle(indexes)\n",
    "\n",
    "train_dataset = train_val_dataset[indexes[:50000]]\n",
    "train_labels = train_val_labels[indexes[:50000]]\n",
    "\n",
    "validation_dataset = train_val_dataset[indexes[50000:]]\n",
    "validation_labels = train_val_labels[indexes[50000:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81da162b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66358c0c",
   "metadata": {},
   "source": [
    "### Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1b2deb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = Scaler_MinMax((-1,1))\n",
    "scaler.min = 0\n",
    "scaler.max = 255\n",
    "train_dataset = scaler.transform(train_dataset)\n",
    "test_dataset = scaler.transform(test_dataset)\n",
    "validation_dataset = scaler.transform(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eef779a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8145680b",
   "metadata": {},
   "source": [
    "### Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eabaef6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch = 10\n",
    "alpha = 0.001\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515d0a1b",
   "metadata": {},
   "source": [
    "### Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c5ac970",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = Layer_Linear(784, 128)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "dropout1 = Layer_Dropout(0.25)\n",
    "\n",
    "layer2 = Layer_Linear(128, 64)\n",
    "activation2 = Activation_ReLU()\n",
    "\n",
    "layer3 = Layer_Linear(64, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ccf4b5",
   "metadata": {},
   "source": [
    "### Initlize optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "329ded87",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = Loss_CategoricalCrossEntropy_Activation_SoftMax()\n",
    "accuracy = Accuracy_Categorical()\n",
    "optimizer = Optimizer_Adam(alpha=alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440c16f0",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e090a644",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_steps = len(train_dataset) // batch_size\n",
    "if train_steps * batch_size < len(train_dataset):\n",
    "    train_steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4e0a9c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_steps = len(validation_dataset) // batch_size\n",
    "if valid_steps * batch_size < len(validation_dataset):\n",
    "    valid_steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5bdc3928",
   "metadata": {},
   "outputs": [],
   "source": [
    "###To track train and valid error\n",
    "train_error_history = []\n",
    "valid_error_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7e6898c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, Train error: 0.757, Train accuracy: 0.720 Validation error: 0.512, Validation accuracy: 0.811\n",
      "epoch: 1, Train error: 0.478, Train accuracy: 0.826 Validation error: 0.448, Validation accuracy: 0.831\n",
      "epoch: 2, Train error: 0.426, Train accuracy: 0.845 Validation error: 0.403, Validation accuracy: 0.849\n",
      "epoch: 3, Train error: 0.394, Train accuracy: 0.858 Validation error: 0.384, Validation accuracy: 0.861\n",
      "epoch: 4, Train error: 0.373, Train accuracy: 0.864 Validation error: 0.372, Validation accuracy: 0.864\n",
      "epoch: 5, Train error: 0.358, Train accuracy: 0.869 Validation error: 0.350, Validation accuracy: 0.873\n",
      "epoch: 6, Train error: 0.343, Train accuracy: 0.875 Validation error: 0.342, Validation accuracy: 0.876\n",
      "epoch: 7, Train error: 0.331, Train accuracy: 0.879 Validation error: 0.341, Validation accuracy: 0.877\n",
      "epoch: 8, Train error: 0.321, Train accuracy: 0.882 Validation error: 0.336, Validation accuracy: 0.879\n",
      "epoch: 9, Train error: 0.312, Train accuracy: 0.886 Validation error: 0.336, Validation accuracy: 0.878\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_epoch):\n",
    "    train_error = 0\n",
    "    valid_error = 0\n",
    "    train_accuracy = 0\n",
    "    valid_accuracy = 0\n",
    "    \n",
    "    for i in range(train_steps):\n",
    "        batch_start = i * batch_size\n",
    "        batch_end = (i+1) * batch_size\n",
    "        \n",
    "        input = train_dataset[batch_start:batch_end]\n",
    "        true = train_labels[batch_start:batch_end]\n",
    "        \n",
    "        #Forward pass\n",
    "        layer1.forward(input)\n",
    "        activation1.forward(layer1.output)\n",
    "        dropout1.forward(activation1.output)\n",
    "        layer2.forward(dropout1.output)\n",
    "        activation2.forward(layer2.output)\n",
    "        layer3.forward(activation2.output)\n",
    "        \n",
    "        train_error += loss.forward(layer3.output, true) / train_steps\n",
    "        train_accuracy += accuracy.calculate(layer3.output, true) / train_steps\n",
    "\n",
    "        #Backward pass\n",
    "        loss.backward(loss.activation.output, true)\n",
    "        layer3.backward(loss.dresults)\n",
    "        activation2.backward(layer3.dresults)\n",
    "        layer2.backward(activation2.dresults)\n",
    "        dropout1.backward(layer2.dresults)\n",
    "        activation1.backward(dropout1.dresults)\n",
    "        layer1.backward(activation1.dresults)\n",
    "\n",
    "        #Update parameters\n",
    "        optimizer.update_parameters(layer3)\n",
    "        optimizer.update_parameters(layer2)\n",
    "        optimizer.update_parameters(layer1)\n",
    "        optimizer.post_update_parameters()\n",
    "    \n",
    "    for i in range(valid_steps):\n",
    "        batch_start = i * batch_size\n",
    "        batch_end = (i+1) * batch_size\n",
    "        \n",
    "        input = validation_dataset[batch_start:batch_end]\n",
    "        true = validation_labels[batch_start:batch_end]\n",
    "        \n",
    "        #Forward pass\n",
    "        layer1.forward(input)\n",
    "        activation1.forward(layer1.output)\n",
    "        layer2.forward(activation1.output)\n",
    "        activation2.forward(layer2.output)\n",
    "        layer3.forward(activation2.output)\n",
    "        valid_error += loss.forward(layer3.output, true) / valid_steps\n",
    "        valid_accuracy += accuracy.calculate(layer3.output, true) / valid_steps\n",
    "    \n",
    "    train_error_history.append(train_error)\n",
    "    valid_error_history.append(valid_error)\n",
    "    print(f'epoch: {epoch},',\n",
    "          f'Train error: {train_error:.3f},',\n",
    "          f'Train accuracy: {train_accuracy:.3f}',\n",
    "          f'Validation error: {valid_error:.3f},',\n",
    "          f'Validation accuracy: {valid_accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1e4f9e83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x28f516aef10>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_error_history, label='training error')\n",
    "plt.plot(valid_error_history, label='validation error')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b236fa4",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3253f918",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_steps = len(test_dataset) // batch_size\n",
    "if test_steps * batch_size < len(test_dataset):\n",
    "    test_steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "66c60452",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_error = 0\n",
    "test_accuracy = 0\n",
    "\n",
    "for i in range(test_steps):\n",
    "    batch_start = i * batch_size\n",
    "    batch_end = (i+1) * batch_size\n",
    "    \n",
    "    input = test_dataset[batch_start:batch_end]\n",
    "    true = test_labels[batch_start:batch_end]\n",
    "    \n",
    "    layer1.forward(input)\n",
    "    activation1.forward(layer1.output)\n",
    "    layer2.forward(activation1.output)\n",
    "    activation2.forward(layer2.output)\n",
    "    layer3.forward(activation2.output)\n",
    "    test_error += loss.forward(layer3.output, true) / test_steps\n",
    "    test_accuracy += accuracy.calculate(layer3.output, true) / test_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d8902639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error: 0.365, Test accuracy: 0.872\n"
     ]
    }
   ],
   "source": [
    "print(f'Test error: {test_error:.3f},',\n",
    "      f'Test accuracy: {test_accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de8f63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GD(without momentum) Test error: 0.375, Test accuracy: 0.869\n",
    "# GD(with momentum)    Test error: 0.367, Test accuracy: 0.873\n",
    "# AdaGrad              Test error: 0.396, Test accuracy: 0.859\n",
    "# RMSprop              Test error: 0.464, Test accuracy: 0.841"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
