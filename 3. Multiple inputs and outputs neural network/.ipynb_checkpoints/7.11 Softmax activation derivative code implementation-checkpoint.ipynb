{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "211f9c01",
   "metadata": {},
   "source": [
    "# Deep Learning - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649f7822",
   "metadata": {},
   "source": [
    "## Chapter 3: Multiple inputs Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592d6d6e",
   "metadata": {},
   "source": [
    "### Softmax activation derivative code implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a24843",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f0ca82",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fca357a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b879cc",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83e1ef8",
   "metadata": {},
   "source": [
    "#### Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d55c27a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"Representing a neural network layer\"\"\"\n",
    "    \n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        \"\"\"Initlize weights and bias\"\"\"\n",
    "        self.weights = np.random.randn(n_inputs, n_outputs)\n",
    "        self.biases = np.zeros((1, n_outputs))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        It multiplies the inputs by the weights \n",
    "        and then sums them, and then sums bias.\n",
    "        \"\"\"\n",
    "        #To calculate gradient, remembering input values\n",
    "        self.inputs = inputs\n",
    "        #Calculate outputs' values\n",
    "        self.outputs = np.dot(inputs, self.weights) + self.biases\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        \"\"\"Gradient with respect to parameters\"\"\"\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09103fd1",
   "metadata": {},
   "source": [
    "### Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149c787f",
   "metadata": {},
   "source": [
    "#### Softmax Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "820a4706",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax:\n",
    "    \"\"\"Softmax activation\"\"\"\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        \n",
    "        #Compute e^x for each element of inputs\n",
    "        exp_values = np.exp(inputs)\n",
    "        \n",
    "        #Normalize them for each batch\n",
    "        self.output = exp_values / np.sum(exp_values, \n",
    "                                          axis=1, keepdims=True)\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        \"\"\"Gradient softmax\"\"\"\n",
    "        \n",
    "        #Initialize an array\n",
    "        self.dresults = np.zeros(dvalues.shape)\n",
    "        \n",
    "        for i in range(len(dvalues)):\n",
    "            #Reshape the single output\n",
    "            single_output = self.output[i].reshape(-1, 1)\n",
    "            \n",
    "            #Calculate Jacobian matrix of the single output\n",
    "            jacobian_matrix = np.diagflat(single_output) - \\\n",
    "                                np.dot(single_output, single_output.T)\n",
    "            \n",
    "            #Multiply the Jacobian matrix by the loss function derivative\n",
    "            self.dresults[i] = np.dot(jacobian_matrix, dvalues[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325b26d3",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b268858f",
   "metadata": {},
   "source": [
    "#### MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5529a656",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_MSE:\n",
    "    \"\"\"MSE Loss function\"\"\"\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"Forward pass\"\"\"     \n",
    "        error = np.mean((y_pred - y_true) ** 2)\n",
    "        return error\n",
    "    \n",
    "    def backward(self, y_pred, y_true):\n",
    "        \"\"\"Derivative of MSE with respect to pred\"\"\"\n",
    "        \n",
    "        #Number of samples\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        #Number of output nodes\n",
    "        outputs = len(y_pred[0])\n",
    "        \n",
    "        #Derivative of MSE\n",
    "        self.dresults = 2 * (y_pred - y_true) / (outputs * samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d926f58",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8a3dfc",
   "metadata": {},
   "source": [
    "#### Gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4d350dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_GD:\n",
    "    \"\"\"Gradient descent optimizer\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=1.):\n",
    "        \"\"\"Initialize hyperparameters\"\"\"\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def update_parameters(self, layer):\n",
    "        \"\"\"Update parameters\"\"\"\n",
    "        weights_delta = layer.dweights * self.alpha\n",
    "        biases_delta = layer.dbiases * self.alpha\n",
    "        \n",
    "        #Update parameters\n",
    "        layer.weights -= weights_delta\n",
    "        layer.biases -= biases_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6ba5dc",
   "metadata": {},
   "source": [
    "### Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9912b52d",
   "metadata": {},
   "source": [
    "#### Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b97eddea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scaler_Standard:\n",
    "    \"\"\"Standard scaler\"\"\"\n",
    "    \n",
    "    def fit(self, data):\n",
    "        \"\"\"Find mean and std values\"\"\"\n",
    "        self.means = data.mean(axis=0)\n",
    "        self.stds = data.std(axis=0)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, data):\n",
    "        \"\"\"Transforming data\"\"\"\n",
    "        return (data - self.means) / self.stds\n",
    "    \n",
    "    def fit_transform(self, data):\n",
    "        \"\"\"Fit and transform data\"\"\"\n",
    "        return self.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c850cf2",
   "metadata": {},
   "source": [
    "#### MinMax Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e5c1fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scaler_MinMax:\n",
    "    \"\"\"MinMax scaler\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_range=(0,1)):\n",
    "        \"\"\"Initialize the feature range\"\"\"\n",
    "        self.low, self.high = feature_range\n",
    "    \n",
    "    def fit(self, data):\n",
    "        \"\"\"Find min and max values\"\"\"\n",
    "        self.min = data.min(axis=0)\n",
    "        self.max = data.max(axis=0)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, data):\n",
    "        \"\"\"Transforming data\"\"\"\n",
    "        data_std = (data - self.min) / (self.max - self.min)\n",
    "        return data_std * (self.high - self.low) + self.low\n",
    "    \n",
    "    def fit_transform(self, data):\n",
    "        \"\"\"Fit and transform data\"\"\"\n",
    "        return self.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa52189",
   "metadata": {},
   "source": [
    "#### Robust Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e867d610",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scaler_Robust:\n",
    "    \"\"\"Robust scaler\"\"\"\n",
    "    \n",
    "    def fit(self, data):\n",
    "        \"\"\"Find median and iqr values\"\"\"\n",
    "        self.medians = np.median(data, axis=0)\n",
    "        self.p75, self.p25 = np.percentile(data, [75 ,25], axis=0)\n",
    "        self.iqr = self.p75 - self.p25\n",
    "        return self\n",
    "    \n",
    "    def transform(self, data):\n",
    "        \"\"\"Transforming data\"\"\"\n",
    "        return (data - self.medians) / self.iqr\n",
    "    \n",
    "    def fit_transform(self, data):\n",
    "        \"\"\"Fit and transform data\"\"\"\n",
    "        return self.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3c0507",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8145680b",
   "metadata": {},
   "source": [
    "### Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eabaef6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch = 300\n",
    "alpha = 0.1\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7059a12c",
   "metadata": {},
   "source": [
    "### Construct Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6abeb1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = np.array([[30, 2],  \n",
    "                          [25, 1],\n",
    "                          [27, 3],\n",
    "                          [23, 4]])\n",
    "train_label = np.array([[1],\n",
    "                        [2], \n",
    "                        [1], \n",
    "                        [3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66358c0c",
   "metadata": {},
   "source": [
    "### Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1b2deb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = Scaler_Standard()\n",
    "train_dataset = scaler.fit_transform(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515d0a1b",
   "metadata": {},
   "source": [
    "### Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c5ac970",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = Layer(2,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ccf4b5",
   "metadata": {},
   "source": [
    "### Initlize optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "720bfce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = Loss_MSE()\n",
    "optimizer = Optimizer_GD(alpha=alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440c16f0",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e090a644",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 \tloss: 7.226202757575061\n",
      "epoch: 1 \tloss: 4.191910679286955\n",
      "epoch: 2 \tloss: 2.478550336353355\n",
      "epoch: 3 \tloss: 1.501185020293256\n",
      "epoch: 4 \tloss: 0.9378064988018244\n",
      "epoch: 5 \tloss: 0.6096312224808851\n",
      "epoch: 6 \tloss: 0.4164754215373168\n",
      "epoch: 7 \tloss: 0.30164604207804424\n",
      "epoch: 8 \tloss: 0.23273097483128322\n",
      "epoch: 9 \tloss: 0.19100499320465075\n",
      "epoch: 10 \tloss: 0.16553623533644013\n",
      "epoch: 11 \tloss: 0.14987676343564932\n",
      "epoch: 12 \tloss: 0.1401856644232585\n",
      "epoch: 13 \tloss: 0.1341535817789223\n",
      "epoch: 14 \tloss: 0.13037998411767962\n",
      "epoch: 15 \tloss: 0.1280088025443155\n",
      "epoch: 16 \tloss: 0.1265130591477164\n",
      "epoch: 17 \tloss: 0.12556631681265878\n",
      "epoch: 18 \tloss: 0.12496524586331217\n",
      "epoch: 19 \tloss: 0.12458258640446696\n",
      "epoch: 20 \tloss: 0.12433835531501614\n",
      "epoch: 21 \tloss: 0.12418210018056933\n",
      "epoch: 22 \tloss: 0.12408189541959572\n",
      "epoch: 23 \tloss: 0.12401748283979741\n",
      "epoch: 24 \tloss: 0.12397597598653856\n",
      "epoch: 25 \tloss: 0.12394915933208116\n",
      "epoch: 26 \tloss: 0.12393178436812943\n",
      "epoch: 27 \tloss: 0.12392049145205207\n",
      "epoch: 28 \tloss: 0.12391312585120129\n",
      "epoch: 29 \tloss: 0.12390830288294451\n",
      "epoch: 30 \tloss: 0.1239051308782757\n",
      "epoch: 31 \tloss: 0.12390303437058918\n",
      "epoch: 32 \tloss: 0.12390164106029508\n",
      "epoch: 33 \tloss: 0.12390070943438522\n",
      "epoch: 34 \tloss: 0.12390008234437552\n",
      "epoch: 35 \tloss: 0.12389965718399942\n",
      "epoch: 36 \tloss: 0.12389936669758732\n",
      "epoch: 37 \tloss: 0.12389916660640074\n",
      "epoch: 38 \tloss: 0.12389902761420252\n",
      "epoch: 39 \tloss: 0.12389893023013049\n",
      "epoch: 40 \tloss: 0.12389886140717166\n",
      "epoch: 41 \tloss: 0.12389881235294199\n",
      "epoch: 42 \tloss: 0.12389877709904447\n",
      "epoch: 43 \tloss: 0.12389875156249622\n",
      "epoch: 44 \tloss: 0.12389873292724457\n",
      "epoch: 45 \tloss: 0.12389871923456981\n",
      "epoch: 46 \tloss: 0.12389870911030511\n",
      "epoch: 47 \tloss: 0.12389870158204652\n",
      "epoch: 48 \tloss: 0.12389869595583157\n",
      "epoch: 49 \tloss: 0.12389869173232873\n",
      "epoch: 50 \tloss: 0.12389868854943095\n",
      "epoch: 51 \tloss: 0.12389868614261251\n",
      "epoch: 52 \tloss: 0.12389868431731917\n",
      "epoch: 53 \tloss: 0.12389868292957307\n",
      "epoch: 54 \tloss: 0.12389868187223047\n",
      "epoch: 55 \tloss: 0.12389868106516223\n",
      "epoch: 56 \tloss: 0.12389868044818017\n",
      "epoch: 57 \tloss: 0.12389867997590145\n",
      "epoch: 58 \tloss: 0.12389867961399306\n",
      "epoch: 59 \tloss: 0.12389867933640736\n",
      "epoch: 60 \tloss: 0.12389867912333367\n",
      "epoch: 61 \tloss: 0.12389867895967381\n",
      "epoch: 62 \tloss: 0.12389867883390078\n",
      "epoch: 63 \tloss: 0.12389867873720027\n",
      "epoch: 64 \tloss: 0.12389867866282425\n",
      "epoch: 65 \tloss: 0.123898678605601\n",
      "epoch: 66 \tloss: 0.12389867856156328\n",
      "epoch: 67 \tloss: 0.12389867852766542\n",
      "epoch: 68 \tloss: 0.12389867850156802\n",
      "epoch: 69 \tloss: 0.12389867848147307\n",
      "epoch: 70 \tloss: 0.12389867846599797\n",
      "epoch: 71 \tloss: 0.12389867845407941\n",
      "epoch: 72 \tloss: 0.12389867844489932\n",
      "epoch: 73 \tloss: 0.12389867843782787\n",
      "epoch: 74 \tloss: 0.12389867843238044\n",
      "epoch: 75 \tloss: 0.12389867842818375\n",
      "epoch: 76 \tloss: 0.12389867842495073\n",
      "epoch: 77 \tloss: 0.12389867842245977\n",
      "epoch: 78 \tloss: 0.12389867842054059\n",
      "epoch: 79 \tloss: 0.12389867841906191\n",
      "epoch: 80 \tloss: 0.1238986784179226\n",
      "epoch: 81 \tloss: 0.12389867841704477\n",
      "epoch: 82 \tloss: 0.12389867841636833\n",
      "epoch: 83 \tloss: 0.12389867841584715\n",
      "epoch: 84 \tloss: 0.12389867841544558\n",
      "epoch: 85 \tloss: 0.12389867841513612\n",
      "epoch: 86 \tloss: 0.12389867841489768\n",
      "epoch: 87 \tloss: 0.12389867841471398\n",
      "epoch: 88 \tloss: 0.12389867841457242\n",
      "epoch: 89 \tloss: 0.1238986784144633\n",
      "epoch: 90 \tloss: 0.12389867841437927\n",
      "epoch: 91 \tloss: 0.12389867841431443\n",
      "epoch: 92 \tloss: 0.12389867841426457\n",
      "epoch: 93 \tloss: 0.12389867841422607\n",
      "epoch: 94 \tloss: 0.12389867841419644\n",
      "epoch: 95 \tloss: 0.12389867841417361\n",
      "epoch: 96 \tloss: 0.12389867841415604\n",
      "epoch: 97 \tloss: 0.12389867841414245\n",
      "epoch: 98 \tloss: 0.12389867841413205\n",
      "epoch: 99 \tloss: 0.12389867841412397\n",
      "epoch: 100 \tloss: 0.12389867841411778\n",
      "epoch: 101 \tloss: 0.12389867841411295\n",
      "epoch: 102 \tloss: 0.1238986784141093\n",
      "epoch: 103 \tloss: 0.12389867841410648\n",
      "epoch: 104 \tloss: 0.12389867841410428\n",
      "epoch: 105 \tloss: 0.12389867841410264\n",
      "epoch: 106 \tloss: 0.12389867841410136\n",
      "epoch: 107 \tloss: 0.12389867841410027\n",
      "epoch: 108 \tloss: 0.12389867841409952\n",
      "epoch: 109 \tloss: 0.12389867841409892\n",
      "epoch: 110 \tloss: 0.12389867841409845\n",
      "epoch: 111 \tloss: 0.12389867841409807\n",
      "epoch: 112 \tloss: 0.12389867841409785\n",
      "epoch: 113 \tloss: 0.12389867841409762\n",
      "epoch: 114 \tloss: 0.12389867841409746\n",
      "epoch: 115 \tloss: 0.1238986784140974\n",
      "epoch: 116 \tloss: 0.12389867841409727\n",
      "epoch: 117 \tloss: 0.12389867841409719\n",
      "epoch: 118 \tloss: 0.12389867841409714\n",
      "epoch: 119 \tloss: 0.12389867841409707\n",
      "epoch: 120 \tloss: 0.12389867841409707\n",
      "epoch: 121 \tloss: 0.12389867841409702\n",
      "epoch: 122 \tloss: 0.12389867841409705\n",
      "epoch: 123 \tloss: 0.12389867841409703\n",
      "epoch: 124 \tloss: 0.12389867841409696\n",
      "epoch: 125 \tloss: 0.12389867841409694\n",
      "epoch: 126 \tloss: 0.12389867841409694\n",
      "epoch: 127 \tloss: 0.12389867841409699\n",
      "epoch: 128 \tloss: 0.1238986784140969\n",
      "epoch: 129 \tloss: 0.12389867841409694\n",
      "epoch: 130 \tloss: 0.12389867841409694\n",
      "epoch: 131 \tloss: 0.12389867841409695\n",
      "epoch: 132 \tloss: 0.12389867841409695\n",
      "epoch: 133 \tloss: 0.12389867841409694\n",
      "epoch: 134 \tloss: 0.12389867841409688\n",
      "epoch: 135 \tloss: 0.12389867841409691\n",
      "epoch: 136 \tloss: 0.12389867841409692\n",
      "epoch: 137 \tloss: 0.12389867841409695\n",
      "epoch: 138 \tloss: 0.12389867841409694\n",
      "epoch: 139 \tloss: 0.12389867841409694\n",
      "epoch: 140 \tloss: 0.12389867841409694\n",
      "epoch: 141 \tloss: 0.12389867841409694\n",
      "epoch: 142 \tloss: 0.12389867841409694\n",
      "epoch: 143 \tloss: 0.12389867841409695\n",
      "epoch: 144 \tloss: 0.12389867841409694\n",
      "epoch: 145 \tloss: 0.12389867841409688\n",
      "epoch: 146 \tloss: 0.12389867841409695\n",
      "epoch: 147 \tloss: 0.12389867841409688\n",
      "epoch: 148 \tloss: 0.12389867841409691\n",
      "epoch: 149 \tloss: 0.12389867841409694\n",
      "epoch: 150 \tloss: 0.12389867841409691\n",
      "epoch: 151 \tloss: 0.12389867841409691\n",
      "epoch: 152 \tloss: 0.12389867841409691\n",
      "epoch: 153 \tloss: 0.12389867841409696\n",
      "epoch: 154 \tloss: 0.12389867841409694\n",
      "epoch: 155 \tloss: 0.12389867841409696\n",
      "epoch: 156 \tloss: 0.12389867841409691\n",
      "epoch: 157 \tloss: 0.12389867841409695\n",
      "epoch: 158 \tloss: 0.12389867841409695\n",
      "epoch: 159 \tloss: 0.12389867841409698\n",
      "epoch: 160 \tloss: 0.12389867841409695\n",
      "epoch: 161 \tloss: 0.12389867841409691\n",
      "epoch: 162 \tloss: 0.12389867841409694\n",
      "epoch: 163 \tloss: 0.12389867841409691\n",
      "epoch: 164 \tloss: 0.12389867841409691\n",
      "epoch: 165 \tloss: 0.12389867841409692\n",
      "epoch: 166 \tloss: 0.12389867841409688\n",
      "epoch: 167 \tloss: 0.1238986784140969\n",
      "epoch: 168 \tloss: 0.12389867841409694\n",
      "epoch: 169 \tloss: 0.12389867841409692\n",
      "epoch: 170 \tloss: 0.12389867841409694\n",
      "epoch: 171 \tloss: 0.12389867841409691\n",
      "epoch: 172 \tloss: 0.12389867841409698\n",
      "epoch: 173 \tloss: 0.12389867841409694\n",
      "epoch: 174 \tloss: 0.12389867841409688\n",
      "epoch: 175 \tloss: 0.12389867841409696\n",
      "epoch: 176 \tloss: 0.1238986784140969\n",
      "epoch: 177 \tloss: 0.12389867841409694\n",
      "epoch: 178 \tloss: 0.12389867841409695\n",
      "epoch: 179 \tloss: 0.12389867841409695\n",
      "epoch: 180 \tloss: 0.12389867841409692\n",
      "epoch: 181 \tloss: 0.12389867841409694\n",
      "epoch: 182 \tloss: 0.12389867841409691\n",
      "epoch: 183 \tloss: 0.12389867841409694\n",
      "epoch: 184 \tloss: 0.1238986784140969\n",
      "epoch: 185 \tloss: 0.12389867841409692\n",
      "epoch: 186 \tloss: 0.12389867841409694\n",
      "epoch: 187 \tloss: 0.12389867841409694\n",
      "epoch: 188 \tloss: 0.12389867841409699\n",
      "epoch: 189 \tloss: 0.12389867841409696\n",
      "epoch: 190 \tloss: 0.12389867841409699\n",
      "epoch: 191 \tloss: 0.12389867841409695\n",
      "epoch: 192 \tloss: 0.12389867841409699\n",
      "epoch: 193 \tloss: 0.12389867841409691\n",
      "epoch: 194 \tloss: 0.12389867841409695\n",
      "epoch: 195 \tloss: 0.12389867841409696\n",
      "epoch: 196 \tloss: 0.12389867841409691\n",
      "epoch: 197 \tloss: 0.12389867841409694\n",
      "epoch: 198 \tloss: 0.12389867841409694\n",
      "epoch: 199 \tloss: 0.12389867841409691\n",
      "epoch: 200 \tloss: 0.12389867841409692\n",
      "epoch: 201 \tloss: 0.12389867841409691\n",
      "epoch: 202 \tloss: 0.12389867841409692\n",
      "epoch: 203 \tloss: 0.12389867841409696\n",
      "epoch: 204 \tloss: 0.12389867841409695\n",
      "epoch: 205 \tloss: 0.12389867841409694\n",
      "epoch: 206 \tloss: 0.12389867841409694\n",
      "epoch: 207 \tloss: 0.12389867841409691\n",
      "epoch: 208 \tloss: 0.12389867841409694\n",
      "epoch: 209 \tloss: 0.12389867841409696\n",
      "epoch: 210 \tloss: 0.12389867841409695\n",
      "epoch: 211 \tloss: 0.12389867841409698\n",
      "epoch: 212 \tloss: 0.12389867841409692\n",
      "epoch: 213 \tloss: 0.12389867841409694\n",
      "epoch: 214 \tloss: 0.123898678414097\n",
      "epoch: 215 \tloss: 0.12389867841409692\n",
      "epoch: 216 \tloss: 0.12389867841409694\n",
      "epoch: 217 \tloss: 0.12389867841409692\n",
      "epoch: 218 \tloss: 0.12389867841409694\n",
      "epoch: 219 \tloss: 0.12389867841409691\n",
      "epoch: 220 \tloss: 0.12389867841409692\n",
      "epoch: 221 \tloss: 0.1238986784140969\n",
      "epoch: 222 \tloss: 0.12389867841409695\n",
      "epoch: 223 \tloss: 0.12389867841409688\n",
      "epoch: 224 \tloss: 0.12389867841409691\n",
      "epoch: 225 \tloss: 0.12389867841409692\n",
      "epoch: 226 \tloss: 0.12389867841409691\n",
      "epoch: 227 \tloss: 0.12389867841409694\n",
      "epoch: 228 \tloss: 0.12389867841409692\n",
      "epoch: 229 \tloss: 0.12389867841409694\n",
      "epoch: 230 \tloss: 0.12389867841409694\n",
      "epoch: 231 \tloss: 0.12389867841409695\n",
      "epoch: 232 \tloss: 0.12389867841409692\n",
      "epoch: 233 \tloss: 0.12389867841409692\n",
      "epoch: 234 \tloss: 0.12389867841409696\n",
      "epoch: 235 \tloss: 0.12389867841409692\n",
      "epoch: 236 \tloss: 0.12389867841409695\n",
      "epoch: 237 \tloss: 0.12389867841409692\n",
      "epoch: 238 \tloss: 0.12389867841409695\n",
      "epoch: 239 \tloss: 0.12389867841409696\n",
      "epoch: 240 \tloss: 0.12389867841409699\n",
      "epoch: 241 \tloss: 0.12389867841409695\n",
      "epoch: 242 \tloss: 0.12389867841409692\n",
      "epoch: 243 \tloss: 0.12389867841409691\n",
      "epoch: 244 \tloss: 0.12389867841409698\n",
      "epoch: 245 \tloss: 0.12389867841409688\n",
      "epoch: 246 \tloss: 0.12389867841409699\n",
      "epoch: 247 \tloss: 0.12389867841409696\n",
      "epoch: 248 \tloss: 0.12389867841409695\n",
      "epoch: 249 \tloss: 0.12389867841409688\n",
      "epoch: 250 \tloss: 0.12389867841409698\n",
      "epoch: 251 \tloss: 0.12389867841409691\n",
      "epoch: 252 \tloss: 0.12389867841409698\n",
      "epoch: 253 \tloss: 0.12389867841409691\n",
      "epoch: 254 \tloss: 0.12389867841409691\n",
      "epoch: 255 \tloss: 0.12389867841409691\n",
      "epoch: 256 \tloss: 0.12389867841409691\n",
      "epoch: 257 \tloss: 0.12389867841409691\n",
      "epoch: 258 \tloss: 0.12389867841409691\n",
      "epoch: 259 \tloss: 0.12389867841409691\n",
      "epoch: 260 \tloss: 0.12389867841409691\n",
      "epoch: 261 \tloss: 0.12389867841409691\n",
      "epoch: 262 \tloss: 0.12389867841409691\n",
      "epoch: 263 \tloss: 0.12389867841409691\n",
      "epoch: 264 \tloss: 0.12389867841409696\n",
      "epoch: 265 \tloss: 0.12389867841409696\n",
      "epoch: 266 \tloss: 0.12389867841409696\n",
      "epoch: 267 \tloss: 0.12389867841409696\n",
      "epoch: 268 \tloss: 0.12389867841409696\n",
      "epoch: 269 \tloss: 0.12389867841409696\n",
      "epoch: 270 \tloss: 0.12389867841409696\n",
      "epoch: 271 \tloss: 0.12389867841409696\n",
      "epoch: 272 \tloss: 0.12389867841409696\n",
      "epoch: 273 \tloss: 0.12389867841409696\n",
      "epoch: 274 \tloss: 0.12389867841409696\n",
      "epoch: 275 \tloss: 0.12389867841409696\n",
      "epoch: 276 \tloss: 0.12389867841409696\n",
      "epoch: 277 \tloss: 0.12389867841409696\n",
      "epoch: 278 \tloss: 0.12389867841409696\n",
      "epoch: 279 \tloss: 0.12389867841409696\n",
      "epoch: 280 \tloss: 0.12389867841409696\n",
      "epoch: 281 \tloss: 0.12389867841409696\n",
      "epoch: 282 \tloss: 0.12389867841409696\n",
      "epoch: 283 \tloss: 0.12389867841409696\n",
      "epoch: 284 \tloss: 0.12389867841409696\n",
      "epoch: 285 \tloss: 0.12389867841409696\n",
      "epoch: 286 \tloss: 0.12389867841409696\n",
      "epoch: 287 \tloss: 0.12389867841409696\n",
      "epoch: 288 \tloss: 0.12389867841409696\n",
      "epoch: 289 \tloss: 0.12389867841409696\n",
      "epoch: 290 \tloss: 0.12389867841409696\n",
      "epoch: 291 \tloss: 0.12389867841409696\n",
      "epoch: 292 \tloss: 0.12389867841409696\n",
      "epoch: 293 \tloss: 0.12389867841409696\n",
      "epoch: 294 \tloss: 0.12389867841409696\n",
      "epoch: 295 \tloss: 0.12389867841409696\n",
      "epoch: 296 \tloss: 0.12389867841409696\n",
      "epoch: 297 \tloss: 0.12389867841409696\n",
      "epoch: 298 \tloss: 0.12389867841409696\n",
      "epoch: 299 \tloss: 0.12389867841409696\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_epoch):\n",
    "    #Forward pass\n",
    "    layer1.forward(train_dataset)\n",
    "    loss = loss_function.forward(layer1.outputs, train_label)\n",
    "    print(f'epoch: {epoch} \\tloss: {loss}')\n",
    "    \n",
    "    #Backward pass\n",
    "    loss_function.backward(layer1.outputs, train_label)\n",
    "    layer1.backward(loss_function.dresults)\n",
    "    \n",
    "    #Update parameters\n",
    "    optimizer.update_parameters(layer1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
