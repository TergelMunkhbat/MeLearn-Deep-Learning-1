{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "211f9c01",
   "metadata": {},
   "source": [
    "# Deep Learning - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649f7822",
   "metadata": {},
   "source": [
    "## Chapter 5. CNN and GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592d6d6e",
   "metadata": {},
   "source": [
    "### Max pooling code implementation from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a24843",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f0ca82",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fca357a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b879cc",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83e1ef8",
   "metadata": {},
   "source": [
    "#### Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d55c27a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Linear:\n",
    "    \"\"\"Representing a neural network layer\"\"\"\n",
    "    \n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        \"\"\"Initlize weights and bias\"\"\"\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_outputs)\n",
    "        self.biases = np.zeros((1, n_outputs))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        It multiplies the inputs by the weights \n",
    "        and then sums them, and then sums bias.\n",
    "        \"\"\"\n",
    "        #To calculate gradient, remembering input values\n",
    "        self.inputs = inputs\n",
    "        #Calculate outputs' values\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        \"\"\"Gradient with respect to parameters and input\"\"\"\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        self.dresults = np.dot(dvalues, self.weights.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05a274b",
   "metadata": {},
   "source": [
    "#### Dropout Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "efe7222d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dropout:\n",
    "    \"\"\"Representing a dropout layer\"\"\"\n",
    "    \n",
    "    def __init__(self, rate):\n",
    "        \"\"\"Initlize the success rate of binomial distribution\"\"\"\n",
    "        self.rate = 1 - rate\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Generate the scaled mask and then\n",
    "        apply the mask to the inputs values\n",
    "        \"\"\"\n",
    "        #Generate the scaled mask\n",
    "        self.scaled_mask = np.random.binomial(1, self.rate,\n",
    "                                             size=inputs.shape) / self.rate\n",
    "        #Calculate outputs' values\n",
    "        self.output = inputs * self.scaled_mask\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        \"\"\"\n",
    "        Gradient with respect to inputs, and then\n",
    "        multiply the dvalues accroding to the chain rule\n",
    "        \"\"\"\n",
    "        self.dresults = self.scaled_mask * dvalues    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d2e750",
   "metadata": {},
   "source": [
    "#### Convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2c8e6ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Convolutional:\n",
    "    \"\"\"Representing a convolutional layer\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1):\n",
    "        \"\"\"Initlize the filters and biases\"\"\"\n",
    "        # KxFxFxC\n",
    "        self.weights = 0.01 * np.random.randn(out_channels,\n",
    "                                              in_channels,\n",
    "                                              kernel_size, \n",
    "                                              kernel_size)\n",
    "        self.biases = np.zeros((out_channels, 1))\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "            \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        #To calculate gradient, remembering input values\n",
    "        self.inputs = inputs\n",
    "        #Batch, channel, height, width\n",
    "        self.in_b, self.in_c, self.in_h, self.in_w = inputs.shape\n",
    "        #Calculate output dimensions\n",
    "        out_dim_h = int((self.in_h - self.kernel_size) / self.stride ) + 1\n",
    "        out_dim_w = int((self.in_w - self.kernel_size) / self.stride ) + 1\n",
    "        #Initialize the output shape\n",
    "        self.output = np.zeros((self.in_b, self.out_channels, out_dim_h, out_dim_w))\n",
    "        #Calculate output's values\n",
    "        for n in range(self.in_b):\n",
    "            for current_filter in range(self.out_channels):\n",
    "                current_y = out_y = 0\n",
    "                while current_y + self.kernel_size <= self.in_h:\n",
    "                    current_x = out_x = 0\n",
    "                    while current_x + self.kernel_size <= self.in_w:\n",
    "                        dot_prod = np.sum(self.weights[current_filter] * \\\n",
    "                                                        inputs[n, :, current_y:current_y+self.kernel_size,\n",
    "                                                              current_x:current_x+self.kernel_size])\n",
    "                        self.output[n, current_filter, out_y, out_x] = dot_prod + self.biases[current_filter]\n",
    "                        current_x += self.stride\n",
    "                        out_x += 1\n",
    "                    current_y += self.stride\n",
    "                    out_y += 1\n",
    "                    \n",
    "    def backward(self, dvalues):\n",
    "        \"\"\"Backward pass\"\"\"\n",
    "        #Initialize the shapes\n",
    "        self.dresults = np.zeros(self.inputs.shape)\n",
    "        self.dweights = np.zeros(self.weights.shape)\n",
    "        self.dbiases = np.zeros(self.biases.shape)\n",
    "        #Calculate gradients with respect to weights, biases, and inputs\n",
    "        for n in range(self.in_b):\n",
    "            for current_filter in range(self.out_channels):\n",
    "                current_y = out_y = 0\n",
    "                while current_y + self.kernel_size <= self.in_h:\n",
    "                    current_x = out_x = 0\n",
    "                    while current_x + self.kernel_size <= self.in_w:\n",
    "                        self.dweights[current_filter] += dvalues[n, current_filter, out_y, out_x] * \\\n",
    "                                                            self.inputs[n, :, current_y:current_y+self.kernel_size,\n",
    "                                                                       current_x:current_x+self.kernel_size]\n",
    "\n",
    "                        self.dresults[n, :, current_y:current_y+self.kernel_size,\n",
    "                                     current_x:current_x+self.kernel_size] += dvalues[n, current_filter, out_y, out_x] * \\\n",
    "                                                                               self.weights[current_filter]\n",
    "                        current_x += self.stride\n",
    "                        out_x += 1\n",
    "                    current_y += self.stride\n",
    "                    out_y += 1\n",
    "                self.dbiases[current_filter] = np.sum(dvalues[n, current_filter])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09103fd1",
   "metadata": {},
   "source": [
    "### Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149c787f",
   "metadata": {},
   "source": [
    "#### Softmax Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "820a4706",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax:\n",
    "    \"\"\"Softmax activation\"\"\"\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        \n",
    "        #Compute e^x for each element of inputs\n",
    "        #Due to the overflow error, \n",
    "        #Maximum value of per sample subtract from each row\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "                                           keepdims=True))\n",
    "        \n",
    "        #Normalize them for each batch\n",
    "        self.output = exp_values / np.sum(exp_values, \n",
    "                                          axis=1, keepdims=True)\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        \"\"\"Gradient softmax\"\"\"\n",
    "        \n",
    "        #Initialize an array\n",
    "        self.dresults = np.zeros(dvalues.shape)\n",
    "        \n",
    "        for i in range(len(dvalues)):\n",
    "            #Reshape the single output\n",
    "            single_output = self.output[i].reshape(-1, 1)\n",
    "            \n",
    "            #Calculate Jacobian matrix of the single output\n",
    "            jacobian_matrix = np.diagflat(single_output) - \\\n",
    "                                np.dot(single_output, single_output.T)\n",
    "            \n",
    "            #Multiply the Jacobian matrix by the loss function derivative\n",
    "            self.dresults[i] = np.dot(jacobian_matrix, dvalues[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e25066",
   "metadata": {},
   "source": [
    "#### ReLU Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "000d3640",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLU:\n",
    "    \"\"\"ReLU activation\"\"\"\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        \n",
    "        #To calculate gradient, remembering input values\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        #Calculate outputs' values\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        \"\"\"Backward pass\"\"\"\n",
    "        \n",
    "        self.dresults = self.inputs > 0\n",
    "        self.dresults = self.dresults * dvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54febde6",
   "metadata": {},
   "source": [
    "### Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa80d36d",
   "metadata": {},
   "source": [
    "#### Max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02f6dc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling_Max:\n",
    "    \"\"\"Representing a maxpool layer\"\"\"\n",
    "    \n",
    "    def __init__(self, kernel_size, stride):\n",
    "        \"\"\"Initlize the kernel size and stride values\"\"\"\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "            \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        #To calculate gradient, remembering input values\n",
    "        self.inputs = inputs\n",
    "        #Batch, channel, height, width\n",
    "        self.in_b, self.in_c, self.in_h, self.in_w = inputs.shape\n",
    "        #Calculate output dimensions\n",
    "        out_dim_h = int((self.in_h - self.kernel_size) / self.stride ) + 1\n",
    "        out_dim_w = int((self.in_w - self.kernel_size) / self.stride ) + 1\n",
    "        #Initialize the output's shape\n",
    "        self.output = np.zeros((self.in_b, self.in_c, out_dim_h, out_dim_w))\n",
    "        #Calculate the output's values\n",
    "        for n in range(self.in_b):\n",
    "            for i in range(self.in_c):\n",
    "                current_y = out_y = 0\n",
    "                while current_y + self.kernel_size <= self.in_h:\n",
    "                    current_x = out_x = 0\n",
    "                    while current_x + self.kernel_size <= self.in_w:\n",
    "                        self.output[n, i, out_y, out_x] = np.max(inputs[n, i, current_y:current_y+self.kernel_size,\n",
    "                                                                       current_x:current_x+self.kernel_size])\n",
    "\n",
    "                        current_x += self.stride\n",
    "                        out_x += 1\n",
    "                    current_y += self.stride\n",
    "                    out_y += 1\n",
    "    \n",
    "    def nanargmax(self, arr):\n",
    "        \"\"\"\n",
    "        Finding indexes of the maximal elements in a given array\n",
    "        \"\"\"\n",
    "        idx = np.argmax(arr)\n",
    "        idxs = np.unravel_index(idx, arr.shape)\n",
    "        return idxs \n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        \"\"\"Backward pass\"\"\"\n",
    "        #Initialize the shapes\n",
    "        self.dresults = np.zeros(self.inputs.shape)\n",
    "        #Calculate gradients with respect to inputs\n",
    "        for n in range(self.in_b):\n",
    "            for i in range(self.in_c):\n",
    "                current_y = out_y = 0\n",
    "                while current_y + self.kernel_size <= self.in_h:\n",
    "                    current_x = out_x = 0\n",
    "                    while current_x + self.kernel_size <= self.in_w:\n",
    "                        (a, b) = self.nanargmax(self.inputs[n, i, current_y:current_y+self.kernel_size,\n",
    "                                                            current_x:current_x+self.kernel_size])\n",
    "\n",
    "                        self.dresults[n, i, current_y+a, current_y+b] = dvalues[n, i, out_y, out_x]\n",
    "\n",
    "                        current_x += self.stride\n",
    "                        out_x += 1\n",
    "                    current_y += self.stride\n",
    "                    out_y += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4dcd240",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([[1, 9, 3],\n",
    "                [4, 7, 61]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d3fbd33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.argmax(arr)\n",
    "idxs = np.unravel_index(idx, arr.shape)\n",
    "idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325b26d3",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b268858f",
   "metadata": {},
   "source": [
    "#### MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5529a656",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_MSE():\n",
    "    \"\"\"MSE Loss function\"\"\"\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"Forward pass\"\"\"     \n",
    "        error = np.mean((y_pred - y_true) ** 2)\n",
    "        return error\n",
    "    \n",
    "    def backward(self, y_pred, y_true):\n",
    "        \"\"\"Derivative of MSE with respect to preds\"\"\"\n",
    "        \n",
    "        #Number of samples\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        #Number of output nodes\n",
    "        outputs = len(y_pred[0])\n",
    "        \n",
    "        #Derivative of MSE\n",
    "        self.dresults = 2 * (y_pred - y_true) / (outputs * samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8e8970",
   "metadata": {},
   "source": [
    "#### Categorical Cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "07e2403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_CategoricalCrossEntropy():\n",
    "    \"\"\"Cross entropy Loss function\"\"\"\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        y_pred += 1e-10\n",
    "        y_pred = np.clip(y_pred, None, 1)\n",
    "        true_prediction = np.sum(y_pred * y_true, axis=1)\n",
    "        error = np.mean(-np.log(true_prediction)) \n",
    "        return error\n",
    "    \n",
    "    def backward(self, y_pred, y_true):\n",
    "        \"\"\"Derivative of CCE with respect to preds\"\"\"\n",
    "        \n",
    "        #Number of samples\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        self.dresults = -y_true / (y_pred * samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38650268",
   "metadata": {},
   "source": [
    "#### Categorical Cross-entropy + Softmax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "24017a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_CategoricalCrossEntropy_Activation_SoftMax:\n",
    "    \"\"\"Cateogircal cross entropy loss and SoftMax function\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize Softmax and CCE loss\"\"\"\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossEntropy()\n",
    "        \n",
    "    def forward(self, inputs, y_true):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        self.activation.forward(inputs)\n",
    "        return self.loss.forward(self.activation.output, y_true)\n",
    "    \n",
    "    def backward(self, y_pred, y_true):\n",
    "        \"\"\"Gradient of Categorical cross entropy + Softmax activation\"\"\"\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        self.dresults = (y_pred - y_true) / samples        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae60fe79",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8e5b134b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy_Categorical:\n",
    "    \"\"\"Accuracy calculation for classification\"\"\"\n",
    "    \n",
    "    def calculate(self, y_pred, y_true):\n",
    "        \"\"\"Calculate the accuracy\"\"\"\n",
    "        \n",
    "        true = np.argmax(y_true, axis=1)\n",
    "        pred = np.argmax(y_pred, axis=1)\n",
    "        comparisons = true == pred\n",
    "        \n",
    "        accuracy = np.mean(comparisons)\n",
    "        \n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d926f58",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8a3dfc",
   "metadata": {},
   "source": [
    "#### Gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e4d350dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_GD:\n",
    "    \"\"\"Gradient descent optimizer\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=1., momentum=0):\n",
    "        \"\"\"Initialize hyperparameters\"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.momentum = momentum\n",
    "\n",
    "    def update_parameters(self, layer):\n",
    "        \"\"\"Update parameters\"\"\"\n",
    "\n",
    "        if self.momentum:\n",
    "            \n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            \n",
    "            weights_delta = self.momentum * layer.weight_momentums + \\\n",
    "                            layer.dweights * self.alpha\n",
    "            biases_delta = self.momentum * layer.bias_momentums + \\\n",
    "                            layer.dbiases * self.alpha\n",
    "            \n",
    "            layer.weight_momentums = weights_delta\n",
    "            layer.bias_momentums = biases_delta\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            weights_delta = layer.dweights * self.alpha\n",
    "            biases_delta = layer.dbiases * self.alpha\n",
    "        \n",
    "        #Update parameters\n",
    "        layer.weights -= weights_delta\n",
    "        layer.biases -= biases_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87dc2bc",
   "metadata": {},
   "source": [
    "#### AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5c0ed23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_AdaGrad:\n",
    "    \"\"\"Gradient descent optimizer\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=1., epsilon=1e-10):\n",
    "        \"\"\"Initialize hyperparameters\"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def update_parameters(self, layer):\n",
    "        \"\"\"Update parameters\"\"\"\n",
    "            \n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        layer.weight_cache += layer.dweights ** 2\n",
    "        layer.bias_cache += layer.dbiases ** 2\n",
    "        \n",
    "        weights_delta = layer.dweights * self.alpha / \\\n",
    "                        (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        biases_delta =  layer.dbiases * self.alpha / \\\n",
    "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "        \n",
    "        #Update parameters\n",
    "        layer.weights -= weights_delta\n",
    "        layer.biases -= biases_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cff996",
   "metadata": {},
   "source": [
    "#### RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "90e5922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_RMSprop:\n",
    "    \"\"\"Gradient descent optimizer\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0.01, epsilon=1e-10, rho=0.99):\n",
    "        \"\"\"Initialize hyperparameters\"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "\n",
    "    def update_parameters(self, layer):\n",
    "        \"\"\"Update parameters\"\"\"\n",
    "            \n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        layer.weight_cache = self.rho * layer.weight_cache + \\\n",
    "                        (1 - self.rho) * layer.dweights ** 2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + \\\n",
    "                        (1 - self.rho) * layer.dbiases ** 2\n",
    "        \n",
    "        weights_delta = layer.dweights * self.alpha / \\\n",
    "                        (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        biases_delta =  layer.dbiases * self.alpha / \\\n",
    "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "        \n",
    "        #Update parameters\n",
    "        layer.weights -= weights_delta\n",
    "        layer.biases -= biases_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9fab2d",
   "metadata": {},
   "source": [
    "#### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "30ed509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_Adam:\n",
    "    \"\"\"Gradient descent optimizer\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0.001, epsilon=1e-7,\n",
    "                 beta_1=0.9, beta_2=0.999):\n",
    "        \"\"\"Initialize hyperparameters\"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        self.iterations = 0\n",
    "    \n",
    "    def update_parameters(self, layer):\n",
    "        \"\"\"Update parameters\"\"\"\n",
    "            \n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "        \n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + \\\n",
    "                                (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + \\\n",
    "                                (1 - self.beta_1) * layer.dbiases\n",
    "        \n",
    "        weight_momentums_corrected = layer.weight_momentums / \\\n",
    "                                (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / \\\n",
    "                                (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        \n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + \\\n",
    "                                (1 - self.beta_2) * layer.dweights ** 2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + \\\n",
    "                                (1 - self.beta_2) * layer.dbiases ** 2\n",
    "        \n",
    "        weight_cache_corrected = layer.weight_cache / \\\n",
    "                                (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / \\\n",
    "                                (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        \n",
    "        weights_delta = weight_momentums_corrected * self.alpha / \\\n",
    "                        (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "        biases_delta =  bias_momentums_corrected * self.alpha / \\\n",
    "                        (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "        \n",
    "        #Update parameters\n",
    "        layer.weights -= weights_delta\n",
    "        layer.biases -= biases_delta\n",
    "    \n",
    "    def post_update_parameters(self):\n",
    "        \"\"\"Increase iteration\"\"\"\n",
    "        \n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6ba5dc",
   "metadata": {},
   "source": [
    "### Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9912b52d",
   "metadata": {},
   "source": [
    "#### Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b97eddea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scaler_Standard:\n",
    "    \"\"\"Standard scaler\"\"\"\n",
    "    \n",
    "    def fit(self, data):\n",
    "        \"\"\"Find mean and std values\"\"\"\n",
    "        self.means = data.mean(axis=0)\n",
    "        self.stds = data.std(axis=0)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, data):\n",
    "        \"\"\"Transforming data\"\"\"\n",
    "        return (data - self.means) / self.stds\n",
    "    \n",
    "    def fit_transform(self, data):\n",
    "        \"\"\"Fit and transform data\"\"\"\n",
    "        return self.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c850cf2",
   "metadata": {},
   "source": [
    "#### MinMax Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7e5c1fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scaler_MinMax:\n",
    "    \"\"\"MinMax scaler\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_range=(0,1)):\n",
    "        \"\"\"Initialize the feature range\"\"\"\n",
    "        self.low, self.high = feature_range\n",
    "    \n",
    "    def fit(self, data):\n",
    "        \"\"\"Find min and max values\"\"\"\n",
    "        self.min = data.min(axis=0)\n",
    "        self.max = data.max(axis=0)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, data):\n",
    "        \"\"\"Transforming data\"\"\"\n",
    "        data_std = (data - self.min) / (self.max - self.min)\n",
    "        return data_std * (self.high - self.low) + self.low\n",
    "    \n",
    "    def fit_transform(self, data):\n",
    "        \"\"\"Fit and transform data\"\"\"\n",
    "        return self.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa52189",
   "metadata": {},
   "source": [
    "#### Robust Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e867d610",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scaler_Robust:\n",
    "    \"\"\"Robust scaler\"\"\"\n",
    "    \n",
    "    def fit(self, data):\n",
    "        \"\"\"Find median and iqr values\"\"\"\n",
    "        self.medians = np.median(data, axis=0)\n",
    "        self.p75, self.p25 = np.percentile(data, [75 ,25], axis=0)\n",
    "        self.iqr = self.p75 - self.p25\n",
    "        return self\n",
    "    \n",
    "    def transform(self, data):\n",
    "        \"\"\"Transforming data\"\"\"\n",
    "        return (data - self.medians) / self.iqr\n",
    "    \n",
    "    def fit_transform(self, data):\n",
    "        \"\"\"Fit and transform data\"\"\"\n",
    "        return self.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3c0507",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7059a12c",
   "metadata": {},
   "source": [
    "### Construct Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b843abf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('T-shirt', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "03a14d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    \"\"\"\n",
    "    Load the MNIST fashion dataset\n",
    "    Convert the labels into one-hot vectors\n",
    "    \"\"\"\n",
    "\n",
    "    labels = os.listdir(os.path.join(path))\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    for label in labels:\n",
    "        for file in os.listdir(os.path.join(path, label)):\n",
    "            image = cv2.imread(os.path.join(path, label, file),\n",
    "                                  cv2.IMREAD_UNCHANGED)\n",
    "            X.append(image.reshape(1, 28, 28))\n",
    "            Y.append(label)\n",
    "    \n",
    "    \n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y).astype('uint8')\n",
    "    Y = np.eye(len(labels))[Y].astype('uint8')\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "2b7862d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_dataset, train_val_labels = load_dataset('../dataset/train')\n",
    "test_dataset, test_labels = load_dataset('../dataset/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "a5b886cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 1, 28, 28)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4516efe3",
   "metadata": {},
   "source": [
    "#### Data shuffling and splits to train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "4d885db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = np.array(range(len(train_val_dataset)))\n",
    "np.random.shuffle(indexes)\n",
    "\n",
    "train_dataset = train_val_dataset[indexes[:1000]]\n",
    "train_labels = train_val_labels[indexes[:1000]]\n",
    "\n",
    "validation_dataset = train_val_dataset[indexes[1000:1200]]\n",
    "validation_labels = train_val_labels[indexes[1000:1200]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66358c0c",
   "metadata": {},
   "source": [
    "### Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "d1b2deb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = Scaler_MinMax((-1,1))\n",
    "scaler.min = 0\n",
    "scaler.max = 255\n",
    "train_dataset = scaler.transform(train_dataset)\n",
    "test_dataset = scaler.transform(test_dataset)\n",
    "validation_dataset = scaler.transform(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "eef779a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8145680b",
   "metadata": {},
   "source": [
    "### Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "eabaef6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch = 10\n",
    "alpha = 0.001\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515d0a1b",
   "metadata": {},
   "source": [
    "### Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "4c5ac970",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = Layer_Convolutional(1, 32, 3)\n",
    "activation1 = Activation_ReLU()\n",
    "# 32 x 26 x 26\n",
    "\n",
    "conv2 = Layer_Convolutional(32, 64, 3)\n",
    "activation2 = Activation_ReLU()\n",
    "# 64 x 24 x 24\n",
    "pooling1 = Pooling_Max(2, 2)\n",
    "# 64 x 12 x 12\n",
    "\n",
    "dropout1 = Layer_Dropout(0.25)\n",
    "\n",
    "flatten = Flatten()\n",
    "\n",
    "fc1 = Layer_Linear(9216, 128)\n",
    "activation3 = Activation_ReLU()\n",
    "\n",
    "dropout2 = Layer_Dropout(0.5)\n",
    "\n",
    "fc2 = Layer_Linear(128, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ccf4b5",
   "metadata": {},
   "source": [
    "### Initlize optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "329ded87",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = Loss_CategoricalCrossEntropy_Activation_SoftMax()\n",
    "accuracy = Accuracy_Categorical()\n",
    "optimizer = Optimizer_Adam(alpha=alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440c16f0",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "e090a644",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_steps = len(train_dataset) // batch_size\n",
    "if train_steps * batch_size < len(train_dataset):\n",
    "    train_steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "4e0a9c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_steps = len(validation_dataset) // batch_size\n",
    "if valid_steps * batch_size < len(validation_dataset):\n",
    "    valid_steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "5bdc3928",
   "metadata": {},
   "outputs": [],
   "source": [
    "###To track train and valid error\n",
    "train_error_history = []\n",
    "valid_error_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "7e6898c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, Train error: 2.285, Train accuracy: 0.201 Validation error: 2.213, Validation accuracy: 0.241\n",
      "epoch: 1, Train error: 2.096, Train accuracy: 0.319 Validation error: 1.969, Validation accuracy: 0.434\n",
      "epoch: 2, Train error: 1.835, Train accuracy: 0.442 Validation error: 1.681, Validation accuracy: 0.467\n",
      "epoch: 3, Train error: 1.524, Train accuracy: 0.510 Validation error: 1.278, Validation accuracy: 0.575\n",
      "epoch: 4, Train error: 1.207, Train accuracy: 0.547 Validation error: 1.060, Validation accuracy: 0.628\n",
      "epoch: 5, Train error: 1.074, Train accuracy: 0.604 Validation error: 0.916, Validation accuracy: 0.702\n",
      "epoch: 6, Train error: 0.956, Train accuracy: 0.653 Validation error: 0.869, Validation accuracy: 0.667\n",
      "epoch: 7, Train error: 0.899, Train accuracy: 0.664 Validation error: 0.843, Validation accuracy: 0.726\n",
      "epoch: 8, Train error: 0.843, Train accuracy: 0.692 Validation error: 0.734, Validation accuracy: 0.736\n",
      "epoch: 9, Train error: 0.787, Train accuracy: 0.695 Validation error: 0.729, Validation accuracy: 0.715\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_epoch):\n",
    "    train_error = 0\n",
    "    valid_error = 0\n",
    "    train_accuracy = 0\n",
    "    valid_accuracy = 0\n",
    "    \n",
    "    for i in range(train_steps):\n",
    "        batch_start = i * batch_size\n",
    "        batch_end = (i+1) * batch_size\n",
    "        \n",
    "        input = train_dataset[batch_start:batch_end]\n",
    "        true = train_labels[batch_start:batch_end]\n",
    "        \n",
    "        #Forward pass\n",
    "        conv1.forward(input)\n",
    "        activation1.forward(conv1.output)\n",
    "        conv2.forward(activation1.output)\n",
    "        activation2.forward(conv2.output)\n",
    "        pooling1.forward(activation2.output)\n",
    "        dropout1.forward(pooling1.output)\n",
    "        flatten.forward(dropout1.output)\n",
    "        fc1.forward(flatten.output)\n",
    "        activation3.forward(fc1.output)\n",
    "        dropout2.forward(activation3.output)\n",
    "        fc2.forward(dropout2.output)\n",
    "        \n",
    "        train_error += loss.forward(fc2.output, true) / train_steps\n",
    "        train_accuracy += accuracy.calculate(fc2.output, true) / train_steps\n",
    "\n",
    "        #Backward pass\n",
    "        loss.backward(loss.activation.output, true)\n",
    "        fc2.backward(loss.dresults)\n",
    "        dropout2.backward(fc2.dresults)\n",
    "        activation3.backward(dropout2.dresults)\n",
    "        fc1.backward(activation3.dresults)\n",
    "        flatten.backward(fc1.dresults)\n",
    "        dropout1.backward(flatten.dresults)\n",
    "        pooling1.backward(dropout1.dresults)\n",
    "        activation2.backward(pooling1.dresults)\n",
    "        conv2.backward(activation2.dresults)\n",
    "        activation1.backward(conv2.dresults)\n",
    "        conv1.backward(activation1.dresults)\n",
    "\n",
    "        #Update parameters\n",
    "        optimizer.update_parameters(conv1)\n",
    "        optimizer.update_parameters(conv2)\n",
    "        optimizer.update_parameters(fc1)\n",
    "        optimizer.update_parameters(fc2)\n",
    "        optimizer.post_update_parameters()\n",
    "    \n",
    "    for i in range(valid_steps):\n",
    "        batch_start = i * batch_size\n",
    "        batch_end = (i+1) * batch_size\n",
    "        \n",
    "        input = validation_dataset[batch_start:batch_end]\n",
    "        true = validation_labels[batch_start:batch_end]\n",
    "        \n",
    "        #Forward pass\n",
    "        conv1.forward(input)\n",
    "        activation1.forward(conv1.output)\n",
    "        conv2.forward(activation1.output)\n",
    "        activation2.forward(conv2.output)\n",
    "        pooling1.forward(activation2.output)\n",
    "        flatten.forward(pooling1.output)\n",
    "        fc1.forward(flatten.output)\n",
    "        activation3.forward(fc1.output)\n",
    "        fc2.forward(activation3.output)\n",
    "        valid_error += loss.forward(fc2.output, true) / valid_steps\n",
    "        valid_accuracy += accuracy.calculate(fc2.output, true) / valid_steps\n",
    "\n",
    "    \n",
    "    train_error_history.append(train_error)\n",
    "    valid_error_history.append(valid_error)\n",
    "    print(f'epoch: {epoch},',\n",
    "          f'Train error: {train_error:.3f},',\n",
    "          f'Train accuracy: {train_accuracy:.3f}',\n",
    "          f'Validation error: {valid_error:.3f},',\n",
    "          f'Validation accuracy: {valid_accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "1e4f9e83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2a704b9ee50>"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_error_history, label='training error')\n",
    "plt.plot(valid_error_history, label='validation error')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
