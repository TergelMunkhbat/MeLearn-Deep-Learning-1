{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "211f9c01",
   "metadata": {},
   "source": [
    "# Deep Learning - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649f7822",
   "metadata": {},
   "source": [
    "## Chapter 4: Convolutional neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592d6d6e",
   "metadata": {},
   "source": [
    "### Convolutional nerural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a24843",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f0ca82",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fca357a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b879cc",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83e1ef8",
   "metadata": {},
   "source": [
    "#### Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d55c27a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Linear:\n",
    "    \"\"\"Representing a neural network layer\"\"\"\n",
    "    \n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        \"\"\"Initlize weights and bias\"\"\"\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_outputs)\n",
    "        self.biases = np.zeros((1, n_outputs))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        It multiplies the inputs by the weights \n",
    "        and then sums them, and then sums bias.\n",
    "        \"\"\"\n",
    "        #To calculate gradient, remembering input values\n",
    "        self.inputs = inputs\n",
    "        #Calculate outputs' values\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        \"\"\"Gradient with respect to parameters and input\"\"\"\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        self.dresults = np.dot(dvalues, self.weights.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05a274b",
   "metadata": {},
   "source": [
    "#### Dropout Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efe7222d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dropout:\n",
    "    \"\"\"Representing a dropout layer\"\"\"\n",
    "    \n",
    "    def __init__(self, rate):\n",
    "        \"\"\"Initlize the success rate of binomial distribution\"\"\"\n",
    "        self.rate = 1 - rate\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Generate the scaled mask and then\n",
    "        apply the mask to the inputs values\n",
    "        \"\"\"\n",
    "        #Generate the scaled mask\n",
    "        self.scaled_mask = np.random.binomial(1, self.rate,\n",
    "                                             size=inputs.shape) / self.rate\n",
    "        #Calculate outputs' values\n",
    "        self.output = inputs * self.scaled_mask\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        \"\"\"\n",
    "        Gradient with respect to inputs, and then\n",
    "        multiply the dvalues accroding to the chain rule\n",
    "        \"\"\"\n",
    "        self.dresults = self.scaled_mask * dvalues    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d2e750",
   "metadata": {},
   "source": [
    "#### Convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c8e6ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Convolutional:\n",
    "    \"\"\"Representing a convolutional layer\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1):\n",
    "        \"\"\"Initlize the filters and biases\"\"\"\n",
    "        # KxFxFxC\n",
    "        self.weights = 0.01 * np.random.randn(out_channels,\n",
    "                                              in_channels,\n",
    "                                              kernel_size, \n",
    "                                              kernel_size)\n",
    "        self.biases = np.zeros((out_channels, 1))\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "            \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        #To calculate gradient, remembering input values\n",
    "        self.inputs = inputs\n",
    "        #Batch, channel, height, width\n",
    "        self.in_b, self.in_c, self.in_h, self.in_w = inputs.shape\n",
    "        #Calculate output dimensions\n",
    "        out_dim_h = int((self.in_h - self.kernel_size) / self.stride ) + 1\n",
    "        out_dim_w = int((self.in_w - self.kernel_size) / self.stride ) + 1\n",
    "        #Initialize the output shape\n",
    "        self.output = np.zeros((self.in_b, self.out_channels, out_dim_h, out_dim_w))\n",
    "        #Calculate output's values\n",
    "        for n in range(self.in_b):\n",
    "            for current_filter in range(self.out_channels):\n",
    "                current_y = out_y = 0\n",
    "                while current_y + self.kernel_size <= self.in_h:\n",
    "                    current_x = out_x = 0\n",
    "                    while current_x + self.kernel_size <= self.in_w:\n",
    "                        dot_prod = np.sum(self.weights[current_filter] * \\\n",
    "                                                        inputs[n, :, current_y:current_y+self.kernel_size,\n",
    "                                                              current_x:current_x+self.kernel_size])\n",
    "                        self.output[n, current_filter, out_y, out_x] = dot_prod + self.biases[current_filter]\n",
    "                        current_x += self.stride\n",
    "                        out_x += 1\n",
    "                    current_y += self.stride\n",
    "                    out_y += 1\n",
    "                    \n",
    "    def backward(self, dvalues):\n",
    "        \"\"\"Backward pass\"\"\"\n",
    "        #Initialize the shapes\n",
    "        self.dresults = np.zeros(self.inputs.shape)\n",
    "        self.dweights = np.zeros(self.weights.shape)\n",
    "        self.dbiases = np.zeros(self.biases.shape)\n",
    "        #Calculate gradients with respect to weights, biases, and inputs\n",
    "        for n in range(self.in_b):\n",
    "            for current_filter in range(self.out_channels):\n",
    "                current_y = out_y = 0\n",
    "                while current_y + self.kernel_size <= self.in_h:\n",
    "                    current_x = out_x = 0\n",
    "                    while current_x + self.kernel_size <= self.in_w:\n",
    "                        self.dweights[current_filter] += dvalues[n, current_filter, out_y, out_x] * \\\n",
    "                                                            self.inputs[n, :, current_y:current_y+self.kernel_size,\n",
    "                                                                       current_x:current_x+self.kernel_size]\n",
    "\n",
    "                        self.dresults[n, :, current_y:current_y+self.kernel_size,\n",
    "                                     current_x:current_x+self.kernel_size] += dvalues[n, current_filter, out_y, out_x] * \\\n",
    "                                                                               self.weights[current_filter]\n",
    "                        current_x += self.stride\n",
    "                        out_x += 1\n",
    "                    current_y += self.stride\n",
    "                    out_y += 1\n",
    "                self.dbiases[current_filter] += np.sum(dvalues[n, current_filter])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09103fd1",
   "metadata": {},
   "source": [
    "### Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149c787f",
   "metadata": {},
   "source": [
    "#### Softmax Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "820a4706",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax:\n",
    "    \"\"\"Softmax activation\"\"\"\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        \n",
    "        #Compute e^x for each element of inputs\n",
    "        #Due to the overflow error, \n",
    "        #Maximum value of per sample subtract from each row\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "                                           keepdims=True))\n",
    "        \n",
    "        #Normalize them for each batch\n",
    "        self.output = exp_values / np.sum(exp_values, \n",
    "                                          axis=1, keepdims=True)\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        \"\"\"Gradient softmax\"\"\"\n",
    "        \n",
    "        #Initialize an array\n",
    "        self.dresults = np.zeros(dvalues.shape)\n",
    "        \n",
    "        for i in range(len(dvalues)):\n",
    "            #Reshape the single output\n",
    "            single_output = self.output[i].reshape(-1, 1)\n",
    "            \n",
    "            #Calculate Jacobian matrix of the single output\n",
    "            jacobian_matrix = np.diagflat(single_output) - \\\n",
    "                                np.dot(single_output, single_output.T)\n",
    "            \n",
    "            #Multiply the Jacobian matrix by the loss function derivative\n",
    "            self.dresults[i] = np.dot(jacobian_matrix, dvalues[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e25066",
   "metadata": {},
   "source": [
    "#### ReLU Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "000d3640",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLU:\n",
    "    \"\"\"ReLU activation\"\"\"\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        \n",
    "        #To calculate gradient, remembering input values\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        #Calculate outputs' values\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        \"\"\"Backward pass\"\"\"\n",
    "        \n",
    "        self.dresults = self.inputs > 0\n",
    "        self.dresults = self.dresults * dvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54febde6",
   "metadata": {},
   "source": [
    "### Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa80d36d",
   "metadata": {},
   "source": [
    "#### Max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02f6dc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling_Max:\n",
    "    \"\"\"Representing a maxpool layer\"\"\"\n",
    "    \n",
    "    def __init__(self, kernel_size, stride):\n",
    "        \"\"\"Initlize the kernel size and stride values\"\"\"\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "            \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        #To calculate gradient, remembering input values\n",
    "        self.inputs = inputs\n",
    "        #Batch, channel, height, width\n",
    "        self.in_b, self.in_c, self.in_h, self.in_w = inputs.shape\n",
    "        #Calculate output dimensions\n",
    "        out_dim_h = int((self.in_h - self.kernel_size) / self.stride ) + 1\n",
    "        out_dim_w = int((self.in_w - self.kernel_size) / self.stride ) + 1\n",
    "        #Initialize the output's shape\n",
    "        self.output = np.zeros((self.in_b, self.in_c, out_dim_h, out_dim_w))\n",
    "        #Calculate the output's values\n",
    "        for n in range(self.in_b):\n",
    "            for i in range(self.in_c):\n",
    "                current_y = out_y = 0\n",
    "                while current_y + self.kernel_size <= self.in_h:\n",
    "                    current_x = out_x = 0\n",
    "                    while current_x + self.kernel_size <= self.in_w:\n",
    "                        self.output[n, i, out_y, out_x] = np.max(inputs[n, i, current_y:current_y+self.kernel_size,\n",
    "                                                                       current_x:current_x+self.kernel_size])\n",
    "\n",
    "                        current_x += self.stride\n",
    "                        out_x += 1\n",
    "                    current_y += self.stride\n",
    "                    out_y += 1\n",
    "    \n",
    "    def nanargmax(self, arr):\n",
    "        \"\"\"\n",
    "        Finding indexes of the maximal elements in a given array\n",
    "        \"\"\"\n",
    "        idx = np.argmax(arr)\n",
    "        idxs = np.unravel_index(idx, arr.shape)\n",
    "        return idxs \n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        \"\"\"Backward pass\"\"\"\n",
    "        #Initialize the shapes\n",
    "        self.dresults = np.zeros(self.inputs.shape)\n",
    "        #Calculate gradients with respect to inputs\n",
    "        for n in range(self.in_b):\n",
    "            for i in range(self.in_c):\n",
    "                current_y = out_y = 0\n",
    "                while current_y + self.kernel_size <= self.in_h:\n",
    "                    current_x = out_x = 0\n",
    "                    while current_x + self.kernel_size <= self.in_w:\n",
    "                        (a, b) = self.nanargmax(self.inputs[n, i, current_y:current_y+self.kernel_size,\n",
    "                                                            current_x:current_x+self.kernel_size])\n",
    "\n",
    "                        self.dresults[n, i, current_y+a, current_y+b] = dvalues[n, i, out_y, out_x]\n",
    "\n",
    "                        current_x += self.stride\n",
    "                        out_x += 1\n",
    "                    current_y += self.stride\n",
    "                    out_y += 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c097156",
   "metadata": {},
   "source": [
    "### Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5917cfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten:\n",
    "    \"\"\"Flattening the input\"\"\"\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        \n",
    "        self.in_b, self.in_c, self.in_h, self.in_w = inputs.shape\n",
    "        \n",
    "        self.output = inputs.reshape(self.in_b, self.in_c * self.in_h * self.in_w)\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        \"\"\"Backward pass\"\"\"\n",
    "        \n",
    "        self.dresults = dvalues.reshape(self.in_b, self.in_c, self.in_h, self.in_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325b26d3",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b268858f",
   "metadata": {},
   "source": [
    "#### MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5529a656",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_MSE():\n",
    "    \"\"\"MSE Loss function\"\"\"\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"Forward pass\"\"\"     \n",
    "        error = np.mean((y_pred - y_true) ** 2)\n",
    "        return error\n",
    "    \n",
    "    def backward(self, y_pred, y_true):\n",
    "        \"\"\"Derivative of MSE with respect to preds\"\"\"\n",
    "        \n",
    "        #Number of samples\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        #Number of output nodes\n",
    "        outputs = len(y_pred[0])\n",
    "        \n",
    "        #Derivative of MSE\n",
    "        self.dresults = 2 * (y_pred - y_true) / (outputs * samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8e8970",
   "metadata": {},
   "source": [
    "#### Categorical Cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07e2403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_CategoricalCrossEntropy():\n",
    "    \"\"\"Cross entropy Loss function\"\"\"\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        y_pred += 1e-10\n",
    "        y_pred = np.clip(y_pred, None, 1)\n",
    "        true_prediction = np.sum(y_pred * y_true, axis=1)\n",
    "        error = np.mean(-np.log(true_prediction)) \n",
    "        return error\n",
    "    \n",
    "    def backward(self, y_pred, y_true):\n",
    "        \"\"\"Derivative of CCE with respect to preds\"\"\"\n",
    "        \n",
    "        #Number of samples\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        self.dresults = -y_true / (y_pred * samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38650268",
   "metadata": {},
   "source": [
    "#### Categorical Cross-entropy + Softmax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24017a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_CategoricalCrossEntropy_Activation_SoftMax:\n",
    "    \"\"\"Cateogircal cross entropy loss and SoftMax function\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize Softmax and CCE loss\"\"\"\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossEntropy()\n",
    "        \n",
    "    def forward(self, inputs, y_true):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        self.activation.forward(inputs)\n",
    "        return self.loss.forward(self.activation.output, y_true)\n",
    "    \n",
    "    def backward(self, y_pred, y_true):\n",
    "        \"\"\"Gradient of Categorical cross entropy + Softmax activation\"\"\"\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        self.dresults = (y_pred - y_true) / samples        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae60fe79",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e5b134b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy_Categorical:\n",
    "    \"\"\"Accuracy calculation for classification\"\"\"\n",
    "    \n",
    "    def calculate(self, y_pred, y_true):\n",
    "        \"\"\"Calculate the accuracy\"\"\"\n",
    "        \n",
    "        true = np.argmax(y_true, axis=1)\n",
    "        pred = np.argmax(y_pred, axis=1)\n",
    "        comparisons = true == pred\n",
    "        \n",
    "        accuracy = np.mean(comparisons)\n",
    "        \n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d926f58",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8a3dfc",
   "metadata": {},
   "source": [
    "#### Gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4d350dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_GD:\n",
    "    \"\"\"Gradient descent optimizer\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=1., momentum=0):\n",
    "        \"\"\"Initialize hyperparameters\"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.momentum = momentum\n",
    "\n",
    "    def update_parameters(self, layer):\n",
    "        \"\"\"Update parameters\"\"\"\n",
    "\n",
    "        if self.momentum:\n",
    "            \n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            \n",
    "            weights_delta = self.momentum * layer.weight_momentums + \\\n",
    "                            layer.dweights * self.alpha\n",
    "            biases_delta = self.momentum * layer.bias_momentums + \\\n",
    "                            layer.dbiases * self.alpha\n",
    "            \n",
    "            layer.weight_momentums = weights_delta\n",
    "            layer.bias_momentums = biases_delta\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            weights_delta = layer.dweights * self.alpha\n",
    "            biases_delta = layer.dbiases * self.alpha\n",
    "        \n",
    "        #Update parameters\n",
    "        layer.weights -= weights_delta\n",
    "        layer.biases -= biases_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87dc2bc",
   "metadata": {},
   "source": [
    "#### AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c0ed23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_AdaGrad:\n",
    "    \"\"\"Gradient descent optimizer\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=1., epsilon=1e-10):\n",
    "        \"\"\"Initialize hyperparameters\"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def update_parameters(self, layer):\n",
    "        \"\"\"Update parameters\"\"\"\n",
    "            \n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        layer.weight_cache += layer.dweights ** 2\n",
    "        layer.bias_cache += layer.dbiases ** 2\n",
    "        \n",
    "        weights_delta = layer.dweights * self.alpha / \\\n",
    "                        (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        biases_delta =  layer.dbiases * self.alpha / \\\n",
    "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "        \n",
    "        #Update parameters\n",
    "        layer.weights -= weights_delta\n",
    "        layer.biases -= biases_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cff996",
   "metadata": {},
   "source": [
    "#### RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "90e5922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_RMSprop:\n",
    "    \"\"\"Gradient descent optimizer\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0.01, epsilon=1e-10, rho=0.99):\n",
    "        \"\"\"Initialize hyperparameters\"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "\n",
    "    def update_parameters(self, layer):\n",
    "        \"\"\"Update parameters\"\"\"\n",
    "            \n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        layer.weight_cache = self.rho * layer.weight_cache + \\\n",
    "                        (1 - self.rho) * layer.dweights ** 2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + \\\n",
    "                        (1 - self.rho) * layer.dbiases ** 2\n",
    "        \n",
    "        weights_delta = layer.dweights * self.alpha / \\\n",
    "                        (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        biases_delta =  layer.dbiases * self.alpha / \\\n",
    "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "        \n",
    "        #Update parameters\n",
    "        layer.weights -= weights_delta\n",
    "        layer.biases -= biases_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9fab2d",
   "metadata": {},
   "source": [
    "#### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30ed509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_Adam:\n",
    "    \"\"\"Gradient descent optimizer\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0.001, epsilon=1e-7,\n",
    "                 beta_1=0.9, beta_2=0.999):\n",
    "        \"\"\"Initialize hyperparameters\"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        self.iterations = 0\n",
    "    \n",
    "    def update_parameters(self, layer):\n",
    "        \"\"\"Update parameters\"\"\"\n",
    "            \n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "        \n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + \\\n",
    "                                (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + \\\n",
    "                                (1 - self.beta_1) * layer.dbiases\n",
    "        \n",
    "        weight_momentums_corrected = layer.weight_momentums / \\\n",
    "                                (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / \\\n",
    "                                (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        \n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + \\\n",
    "                                (1 - self.beta_2) * layer.dweights ** 2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + \\\n",
    "                                (1 - self.beta_2) * layer.dbiases ** 2\n",
    "        \n",
    "        weight_cache_corrected = layer.weight_cache / \\\n",
    "                                (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / \\\n",
    "                                (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        \n",
    "        weights_delta = weight_momentums_corrected * self.alpha / \\\n",
    "                        (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "        biases_delta =  bias_momentums_corrected * self.alpha / \\\n",
    "                        (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "        \n",
    "        #Update parameters\n",
    "        layer.weights -= weights_delta\n",
    "        layer.biases -= biases_delta\n",
    "    \n",
    "    def post_update_parameters(self):\n",
    "        \"\"\"Increase iteration\"\"\"\n",
    "        \n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6ba5dc",
   "metadata": {},
   "source": [
    "### Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9912b52d",
   "metadata": {},
   "source": [
    "#### Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b97eddea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scaler_Standard:\n",
    "    \"\"\"Standard scaler\"\"\"\n",
    "    \n",
    "    def fit(self, data):\n",
    "        \"\"\"Find mean and std values\"\"\"\n",
    "        self.means = data.mean(axis=0)\n",
    "        self.stds = data.std(axis=0)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, data):\n",
    "        \"\"\"Transforming data\"\"\"\n",
    "        return (data - self.means) / self.stds\n",
    "    \n",
    "    def fit_transform(self, data):\n",
    "        \"\"\"Fit and transform data\"\"\"\n",
    "        return self.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c850cf2",
   "metadata": {},
   "source": [
    "#### MinMax Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7e5c1fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scaler_MinMax:\n",
    "    \"\"\"MinMax scaler\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_range=(0,1)):\n",
    "        \"\"\"Initialize the feature range\"\"\"\n",
    "        self.low, self.high = feature_range\n",
    "    \n",
    "    def fit(self, data):\n",
    "        \"\"\"Find min and max values\"\"\"\n",
    "        self.min = data.min(axis=0)\n",
    "        self.max = data.max(axis=0)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, data):\n",
    "        \"\"\"Transforming data\"\"\"\n",
    "        data_std = (data - self.min) / (self.max - self.min)\n",
    "        return data_std * (self.high - self.low) + self.low\n",
    "    \n",
    "    def fit_transform(self, data):\n",
    "        \"\"\"Fit and transform data\"\"\"\n",
    "        return self.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa52189",
   "metadata": {},
   "source": [
    "#### Robust Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e867d610",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scaler_Robust:\n",
    "    \"\"\"Robust scaler\"\"\"\n",
    "    \n",
    "    def fit(self, data):\n",
    "        \"\"\"Find median and iqr values\"\"\"\n",
    "        self.medians = np.median(data, axis=0)\n",
    "        self.p75, self.p25 = np.percentile(data, [75 ,25], axis=0)\n",
    "        self.iqr = self.p75 - self.p25\n",
    "        return self\n",
    "    \n",
    "    def transform(self, data):\n",
    "        \"\"\"Transforming data\"\"\"\n",
    "        return (data - self.medians) / self.iqr\n",
    "    \n",
    "    def fit_transform(self, data):\n",
    "        \"\"\"Fit and transform data\"\"\"\n",
    "        return self.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3c0507",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7059a12c",
   "metadata": {},
   "source": [
    "### Construct Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b843abf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('T-shirt', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "03a14d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    \"\"\"\n",
    "    Load the MNIST fashion dataset\n",
    "    Convert the labels into one-hot vectors\n",
    "    \"\"\"\n",
    "\n",
    "    labels = os.listdir(os.path.join(path))\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    for label in labels:\n",
    "        for file in os.listdir(os.path.join(path, label)):\n",
    "            image = cv2.imread(os.path.join(path, label, file),\n",
    "                                  cv2.IMREAD_UNCHANGED)\n",
    "            X.append(image.reshape(1, 28, 28))\n",
    "            Y.append(label)\n",
    "    \n",
    "    \n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y).astype('uint8')\n",
    "    Y = np.eye(len(labels))[Y].astype('uint8')\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2b7862d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_dataset, train_val_labels = load_dataset('../dataset/train')\n",
    "test_dataset, test_labels = load_dataset('../dataset/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a5b886cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 1, 28, 28)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4516efe3",
   "metadata": {},
   "source": [
    "#### Data shuffling and splits to train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4d885db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = np.array(range(len(train_val_dataset)))\n",
    "np.random.shuffle(indexes)\n",
    "\n",
    "train_dataset = train_val_dataset[indexes[:1000]]\n",
    "train_labels = train_val_labels[indexes[:1000]]\n",
    "\n",
    "validation_dataset = train_val_dataset[indexes[1000:1200]]\n",
    "validation_labels = train_val_labels[indexes[1000:1200]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66358c0c",
   "metadata": {},
   "source": [
    "### Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d1b2deb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = Scaler_MinMax((-1,1))\n",
    "scaler.min = 0\n",
    "scaler.max = 255\n",
    "train_dataset = scaler.transform(train_dataset)\n",
    "test_dataset = scaler.transform(test_dataset)\n",
    "validation_dataset = scaler.transform(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eef779a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8145680b",
   "metadata": {},
   "source": [
    "### Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eabaef6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch = 10\n",
    "alpha = 0.001\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515d0a1b",
   "metadata": {},
   "source": [
    "### Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4c5ac970",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = Layer_Convolutional(1, 32, 3)\n",
    "activation1 = Activation_ReLU()\n",
    "# 32 x 26 x 26\n",
    "\n",
    "conv2 = Layer_Convolutional(32, 64, 3)\n",
    "activation2 = Activation_ReLU()\n",
    "# 64 x 24 x 24\n",
    "pooling1 = Pooling_Max(2, 2)\n",
    "# 64 x 12 x 12\n",
    "\n",
    "dropout1 = Layer_Dropout(0.25)\n",
    "\n",
    "flatten = Flatten()\n",
    "\n",
    "fc1 = Layer_Linear(9216, 128)\n",
    "activation3 = Activation_ReLU()\n",
    "\n",
    "dropout2 = Layer_Dropout(0.5)\n",
    "\n",
    "fc2 = Layer_Linear(128, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ccf4b5",
   "metadata": {},
   "source": [
    "### Initlize optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "329ded87",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = Loss_CategoricalCrossEntropy_Activation_SoftMax()\n",
    "accuracy = Accuracy_Categorical()\n",
    "optimizer = Optimizer_Adam(alpha=alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440c16f0",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e090a644",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_steps = len(train_dataset) // batch_size\n",
    "if train_steps * batch_size < len(train_dataset):\n",
    "    train_steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4e0a9c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_steps = len(validation_dataset) // batch_size\n",
    "if valid_steps * batch_size < len(validation_dataset):\n",
    "    valid_steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5bdc3928",
   "metadata": {},
   "outputs": [],
   "source": [
    "###To track train and valid error\n",
    "train_error_history = []\n",
    "valid_error_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7e6898c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, Train error: 2.294, Train accuracy: 0.137 Validation error: 2.253, Validation accuracy: 0.181\n",
      "epoch: 1, Train error: 2.080, Train accuracy: 0.308 Validation error: 1.773, Validation accuracy: 0.406\n",
      "epoch: 2, Train error: 1.533, Train accuracy: 0.490 Validation error: 1.232, Validation accuracy: 0.614\n",
      "epoch: 3, Train error: 1.237, Train accuracy: 0.540 Validation error: 1.084, Validation accuracy: 0.631\n",
      "epoch: 4, Train error: 1.092, Train accuracy: 0.600 Validation error: 0.970, Validation accuracy: 0.655\n",
      "epoch: 5, Train error: 1.001, Train accuracy: 0.641 Validation error: 0.877, Validation accuracy: 0.665\n",
      "epoch: 6, Train error: 0.959, Train accuracy: 0.646 Validation error: 0.851, Validation accuracy: 0.687\n",
      "epoch: 7, Train error: 0.891, Train accuracy: 0.671 Validation error: 0.761, Validation accuracy: 0.707\n",
      "epoch: 8, Train error: 0.852, Train accuracy: 0.691 Validation error: 0.751, Validation accuracy: 0.731\n",
      "epoch: 9, Train error: 0.801, Train accuracy: 0.710 Validation error: 0.728, Validation accuracy: 0.737\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_epoch):\n",
    "    train_error = 0\n",
    "    valid_error = 0\n",
    "    train_accuracy = 0\n",
    "    valid_accuracy = 0\n",
    "    \n",
    "    for i in range(train_steps):\n",
    "        batch_start = i * batch_size\n",
    "        batch_end = (i+1) * batch_size\n",
    "        \n",
    "        input = train_dataset[batch_start:batch_end]\n",
    "        true = train_labels[batch_start:batch_end]\n",
    "        \n",
    "        #Forward pass\n",
    "        conv1.forward(input)\n",
    "        activation1.forward(conv1.output)\n",
    "        conv2.forward(activation1.output)\n",
    "        activation2.forward(conv2.output)\n",
    "        pooling1.forward(activation2.output)\n",
    "        dropout1.forward(pooling1.output)\n",
    "        flatten.forward(dropout1.output)\n",
    "        fc1.forward(flatten.output)\n",
    "        activation3.forward(fc1.output)\n",
    "        dropout2.forward(activation3.output)\n",
    "        fc2.forward(dropout2.output)\n",
    "        \n",
    "        train_error += loss.forward(fc2.output, true) / train_steps\n",
    "        train_accuracy += accuracy.calculate(fc2.output, true) / train_steps\n",
    "\n",
    "        #Backward pass\n",
    "        loss.backward(loss.activation.output, true)\n",
    "        fc2.backward(loss.dresults)\n",
    "        dropout2.backward(fc2.dresults)\n",
    "        activation3.backward(dropout2.dresults)\n",
    "        fc1.backward(activation3.dresults)\n",
    "        flatten.backward(fc1.dresults)\n",
    "        dropout1.backward(flatten.dresults)\n",
    "        pooling1.backward(dropout1.dresults)\n",
    "        activation2.backward(pooling1.dresults)\n",
    "        conv2.backward(activation2.dresults)\n",
    "        activation1.backward(conv2.dresults)\n",
    "        conv1.backward(activation1.dresults)\n",
    "\n",
    "        #Update parameters\n",
    "        optimizer.update_parameters(conv1)\n",
    "        optimizer.update_parameters(conv2)\n",
    "        optimizer.update_parameters(fc1)\n",
    "        optimizer.update_parameters(fc2)\n",
    "        optimizer.post_update_parameters()\n",
    "    \n",
    "    for i in range(valid_steps):\n",
    "        batch_start = i * batch_size\n",
    "        batch_end = (i+1) * batch_size\n",
    "        \n",
    "        input = validation_dataset[batch_start:batch_end]\n",
    "        true = validation_labels[batch_start:batch_end]\n",
    "        \n",
    "        #Forward pass\n",
    "        conv1.forward(input)\n",
    "        activation1.forward(conv1.output)\n",
    "        conv2.forward(activation1.output)\n",
    "        activation2.forward(conv2.output)\n",
    "        pooling1.forward(activation2.output)\n",
    "        flatten.forward(pooling1.output)\n",
    "        fc1.forward(flatten.output)\n",
    "        activation3.forward(fc1.output)\n",
    "        fc2.forward(activation3.output)\n",
    "        valid_error += loss.forward(fc2.output, true) / valid_steps\n",
    "        valid_accuracy += accuracy.calculate(fc2.output, true) / valid_steps\n",
    "\n",
    "    \n",
    "    train_error_history.append(train_error)\n",
    "    valid_error_history.append(valid_error)\n",
    "    print(f'epoch: {epoch},',\n",
    "          f'Train error: {train_error:.3f},',\n",
    "          f'Train accuracy: {train_accuracy:.3f}',\n",
    "          f'Validation error: {valid_error:.3f},',\n",
    "          f'Validation accuracy: {valid_accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1e4f9e83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1ce0b5f2820>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_error_history, label='training error')\n",
    "plt.plot(valid_error_history, label='validation error')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
